{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####   hw6pr2digits_modeler \n",
    "+ digits clasification -- and regression -- via NNets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict pixel 42 (regression)...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# digits_cleaned.csv and hw4pr1iris_cleaner.ipynb should be in this folder\n",
    "# \n",
    "filename = 'digits_cleaned.csv'\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (1768, 65)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1768 entries, 0 to 1767\n",
      "Data columns (total 65 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   pix0          1768 non-null   int64\n",
      " 1   pix1          1768 non-null   int64\n",
      " 2   pix2          1768 non-null   int64\n",
      " 3   pix3          1768 non-null   int64\n",
      " 4   pix4          1768 non-null   int64\n",
      " 5   pix5          1768 non-null   int64\n",
      " 6   pix6          1768 non-null   int64\n",
      " 7   pix7          1768 non-null   int64\n",
      " 8   pix8          1768 non-null   int64\n",
      " 9   pix9          1768 non-null   int64\n",
      " 10  pix10         1768 non-null   int64\n",
      " 11  pix11         1768 non-null   int64\n",
      " 12  pix12         1768 non-null   int64\n",
      " 13  pix13         1768 non-null   int64\n",
      " 14  pix14         1768 non-null   int64\n",
      " 15  pix15         1768 non-null   int64\n",
      " 16  pix16         1768 non-null   int64\n",
      " 17  pix17         1768 non-null   int64\n",
      " 18  pix18         1768 non-null   int64\n",
      " 19  pix19         1768 non-null   int64\n",
      " 20  pix20         1768 non-null   int64\n",
      " 21  pix21         1768 non-null   int64\n",
      " 22  pix22         1768 non-null   int64\n",
      " 23  pix23         1768 non-null   int64\n",
      " 24  pix24         1768 non-null   int64\n",
      " 25  pix25         1768 non-null   int64\n",
      " 26  pix26         1768 non-null   int64\n",
      " 27  pix27         1768 non-null   int64\n",
      " 28  pix28         1768 non-null   int64\n",
      " 29  pix29         1768 non-null   int64\n",
      " 30  pix30         1768 non-null   int64\n",
      " 31  pix31         1768 non-null   int64\n",
      " 32  pix32         1768 non-null   int64\n",
      " 33  pix33         1768 non-null   int64\n",
      " 34  pix34         1768 non-null   int64\n",
      " 35  pix35         1768 non-null   int64\n",
      " 36  pix36         1768 non-null   int64\n",
      " 37  pix37         1768 non-null   int64\n",
      " 38  pix38         1768 non-null   int64\n",
      " 39  pix39         1768 non-null   int64\n",
      " 40  pix40         1768 non-null   int64\n",
      " 41  pix41         1768 non-null   int64\n",
      " 42  pix42         1768 non-null   int64\n",
      " 43  pix43         1768 non-null   int64\n",
      " 44  pix44         1768 non-null   int64\n",
      " 45  pix45         1768 non-null   int64\n",
      " 46  pix46         1768 non-null   int64\n",
      " 47  pix47         1768 non-null   int64\n",
      " 48  pix48         1768 non-null   int64\n",
      " 49  pix49         1768 non-null   int64\n",
      " 50  pix50         1768 non-null   int64\n",
      " 51  pix51         1768 non-null   int64\n",
      " 52  pix52         1768 non-null   int64\n",
      " 53  pix53         1768 non-null   int64\n",
      " 54  pix54         1768 non-null   int64\n",
      " 55  pix55         1768 non-null   int64\n",
      " 56  pix56         1768 non-null   int64\n",
      " 57  pix57         1768 non-null   int64\n",
      " 58  pix58         1768 non-null   int64\n",
      " 59  pix59         1768 non-null   int64\n",
      " 60  pix60         1768 non-null   int64\n",
      " 61  pix61         1768 non-null   int64\n",
      " 62  pix62         1768 non-null   int64\n",
      " 63  pix63         1768 non-null   int64\n",
      " 64  actual_digit  1768 non-null   int64\n",
      "dtypes: int64(65)\n",
      "memory usage: 911.6 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix55</th>\n",
       "      <th>pix56</th>\n",
       "      <th>pix57</th>\n",
       "      <th>pix58</th>\n",
       "      <th>pix59</th>\n",
       "      <th>pix60</th>\n",
       "      <th>pix61</th>\n",
       "      <th>pix62</th>\n",
       "      <th>pix63</th>\n",
       "      <th>actual_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix55  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...      0   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...      0   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...      0   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...      0   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...      2   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...      0   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...      0   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...      0   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...      0   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...      0   \n",
       "\n",
       "      pix56  pix57  pix58  pix59  pix60  pix61  pix62  pix63  actual_digit  \n",
       "0         0      0     11     16     15     11      1      0             8  \n",
       "1         0      0      9     12     13      3      0      0             9  \n",
       "2         0      0      1     10     13      3      0      0             0  \n",
       "3         0      0      0      1     13     16      1      0             1  \n",
       "4         0      0      3     11      8     13     12      4             2  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...           ...  \n",
       "1763      0      0      2     14     15      9      0      0             9  \n",
       "1764      0      0      6     16     14      6      0      0             0  \n",
       "1765      0      0      2      9     13      6      0      0             8  \n",
       "1766      0      0      5     12     16     12      0      0             9  \n",
       "1767      0      1      8     12     14     12      1      0             8  \n",
       "\n",
       "[1768 rows x 65 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# different version vary on how to see all rows (adapt to suit your system!)\n",
    "#\n",
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "\n",
    "# let's print the whole dataframe, too  (adapt # of lines, as desired)\n",
    "# pd.options.display.max_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.max_rows = 10   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 10   # None for no limit; default: 10\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63',\n",
      "       'actual_digit'],\n",
      "      dtype='object')\n",
      "\n",
      "COLUMNS[0] is pix0\n",
      "\n",
      "COL_INDEX is {'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'actual_digit': 64}\n",
      "\n",
      "\n",
      "zero maps to 0\n",
      "one maps to 1\n",
      "two maps to 2\n",
      "three maps to 3\n",
      "four maps to 4\n",
      "five maps to 5\n",
      "six maps to 6\n",
      "seven maps to 7\n",
      "eight maps to 8\n",
      "nine maps to 9\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "\n",
    "df_model1= df_tidy  # Makes our lives easier\n",
    "\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['zero','one','two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']   # int to str\n",
    "SPECIES_INDEX = {'zero':0,'one':1,'two':2, 'three':3, 'four':4,'five':5, 'six':6, 'seven':7, 'eight':8, 'nine':9}  # str to int\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  9. ...  1.  0.  8.]\n",
      " [ 0.  0. 11. ...  0.  0.  9.]\n",
      " [ 0.  0.  1. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  0.  0.  8.]\n",
      " [ 0.  0.  2. ...  0.  0.  9.]\n",
      " [ 0.  0. 10. ...  1.  0.  8.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#\n",
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 1768 rows and 65 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit #42 is [ 0.  0.  0. 10. 11.  0.  0.  0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  0.\n",
      " 15. 13.  0.  0.  0.  0.  0.  0. 14. 10.  0.  0.  0.  0.  0.  1. 15. 12.\n",
      "  8.  2.  0.  0.  0.  0. 12. 16. 16. 16. 10.  1.  0.  0.  7. 16. 12. 12.\n",
      " 16.  4.  0.  0.  0.  9. 15. 12.  5.  0.  6.]\n",
      "  Its pix0 is 0.0\n",
      "  Its pix1 is 0.0\n",
      "  Its pix2 is 0.0\n",
      "  Its pix3 is 10.0\n",
      "  Its pix4 is 11.0\n",
      "  Its pix5 is 0.0\n",
      "  Its pix6 is 0.0\n",
      "  Its pix7 is 0.0\n",
      "  Its pix8 is 0.0\n",
      "  Its pix9 is 0.0\n",
      "  Its pix10 is 9.0\n",
      "  Its pix11 is 16.0\n",
      "  Its pix12 is 6.0\n",
      "  Its pix13 is 0.0\n",
      "  Its pix14 is 0.0\n",
      "  Its pix15 is 0.0\n",
      "  Its pix16 is 0.0\n",
      "  Its pix17 is 0.0\n",
      "  Its pix18 is 15.0\n",
      "  Its pix19 is 13.0\n",
      "  Its pix20 is 0.0\n",
      "  Its pix21 is 0.0\n",
      "  Its pix22 is 0.0\n",
      "  Its pix23 is 0.0\n",
      "  Its pix24 is 0.0\n",
      "  Its pix25 is 0.0\n",
      "  Its pix26 is 14.0\n",
      "  Its pix27 is 10.0\n",
      "  Its pix28 is 0.0\n",
      "  Its pix29 is 0.0\n",
      "  Its pix30 is 0.0\n",
      "  Its pix31 is 0.0\n",
      "  Its pix32 is 0.0\n",
      "  Its pix33 is 1.0\n",
      "  Its pix34 is 15.0\n",
      "  Its pix35 is 12.0\n",
      "  Its pix36 is 8.0\n",
      "  Its pix37 is 2.0\n",
      "  Its pix38 is 0.0\n",
      "  Its pix39 is 0.0\n",
      "  Its pix40 is 0.0\n",
      "  Its pix41 is 0.0\n",
      "  Its pix42 is 12.0\n",
      "  Its pix43 is 16.0\n",
      "  Its pix44 is 16.0\n",
      "  Its pix45 is 16.0\n",
      "  Its pix46 is 10.0\n",
      "  Its pix47 is 1.0\n",
      "  Its pix48 is 0.0\n",
      "  Its pix49 is 0.0\n",
      "  Its pix50 is 7.0\n",
      "  Its pix51 is 16.0\n",
      "  Its pix52 is 12.0\n",
      "  Its pix53 is 12.0\n",
      "  Its pix54 is 16.0\n",
      "  Its pix55 is 4.0\n",
      "  Its pix56 is 0.0\n",
      "  Its pix57 is 0.0\n",
      "  Its pix58 is 0.0\n",
      "  Its pix59 is 9.0\n",
      "  Its pix60 is 15.0\n",
      "  Its pix61 is 12.0\n",
      "  Its pix62 is 5.0\n",
      "  Its pix63 is 0.0\n",
      "  Its actual_digit is 6.0\n",
      "  Its species is six (i.e., 6)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce that we have\n",
    "# (1) names...\n",
    "# (2) access and control...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 42\n",
    "print(f\"Digit #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    value = A[n][i]\n",
    "    print(f\"  Its {colname} is {value}\")\n",
    "\n",
    "species_index = COL_INDEX['actual_digit']\n",
    "species_num = int(round(A[n][species_index]))\n",
    "species = SPECIES[species_num]\n",
    "print(f\"  Its species is {species} (i.e., {species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species)   are \n",
      " [8. 9. 0. ... 8. 9. 8.]\n",
      "X_all (just the features, first few rows) are \n",
      " [[ 0.  0.  9. 14.  8.  1.  0.  0.  0.  0. 12. 14. 14. 12.  0.  0.  0.  0.\n",
      "   9. 10.  0. 15.  4.  0.  0.  0.  3. 16. 12. 14.  2.  0.  0.  0.  4. 16.\n",
      "  16.  2.  0.  0.  0.  3. 16.  8. 10. 13.  2.  0.  0.  1. 15.  1.  3. 16.\n",
      "   8.  0.  0.  0. 11. 16. 15. 11.  1.  0.]\n",
      " [ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      "  16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "   9. 15.  2.  0.  0.  0.  0.  3.  0.  9. 11.  0.  0.  0.  0.  0.  9. 15.\n",
      "   4.  0.  0.  0.  9. 12. 13.  3.  0.  0.]\n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      "  16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "   0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.\n",
      "   0.  0.  0.  0.  1. 10. 13.  3.  0.  0.]\n",
      " [ 0.  0.  0.  0. 14. 13.  1.  0.  0.  0.  0.  5. 16. 16.  2.  0.  0.  0.\n",
      "   0. 14. 16. 12.  0.  0.  0.  1. 10. 16. 16. 12.  0.  0.  0.  3. 12. 14.\n",
      "  16.  9.  0.  0.  0.  0.  0.  5. 16. 15.  0.  0.  0.  0.  0.  4. 16. 14.\n",
      "   0.  0.  0.  0.  0.  1. 13. 16.  1.  0.]\n",
      " [ 0.  0.  5. 12.  1.  0.  0.  0.  0.  0. 15. 14.  7.  0.  0.  0.  0.  0.\n",
      "  13.  1. 12.  0.  0.  0.  0.  2. 10.  0. 14.  0.  0.  0.  0.  0.  2.  0.\n",
      "  16.  1.  0.  0.  0.  0.  0.  6. 15.  0.  0.  0.  0.  0.  9. 16. 15.  9.\n",
      "   8.  2.  0.  0.  3. 11.  8. 13. 12.  4.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:64]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,64]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"y_all (just the labels/species)   are \\n {y_all}\")\n",
    "print(f\"X_all (just the features, first few rows) are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [8. 8. 0. ... 9. 7. 9.]\n",
      "The corresponding data rows are \n",
      " [[ 0.  0.  4. 12. 16.  8.  0.  0.  0.  5. 16. 11. 10. 16.  4.  0.  0.  8.\n",
      "  13.  0.  1. 13.  4.  0.  0.  3. 16. 13. 15. 13.  3.  0.  0.  0.  9. 16.\n",
      "  16.  7.  0.  0.  0.  0. 14.  7.  5. 15.  6.  0.  0.  0. 10. 12.  7. 13.\n",
      "  10.  0.  0.  0.  3. 13. 13. 10.  1.  0.]\n",
      " [ 0.  0. 13. 16. 16. 15.  2.  0.  0.  0. 14. 13. 11. 16.  2.  0.  0.  0.\n",
      "  11. 13. 15.  6.  0.  0.  0.  0.  5. 16. 10.  0.  0.  0.  0.  0. 10. 14.\n",
      "  15.  0.  0.  0.  0.  1. 14.  3. 15.  7.  0.  0.  0.  6. 11.  0. 15.  6.\n",
      "   0.  0.  0.  1. 13. 16. 15.  3.  0.  0.]\n",
      " [ 0.  0.  5. 16. 10.  0.  0.  0.  0.  0.  8. 16. 16.  5.  0.  0.  0.  0.\n",
      "  14. 14.  1. 12.  0.  0.  0.  0. 15. 10.  0.  7.  4.  0.  0.  2. 16.  7.\n",
      "   0.  2.  9.  0.  0.  2. 16.  8.  0.  6. 11.  0.  0.  1. 12. 14. 14. 16.\n",
      "   5.  0.  0.  0.  4. 15. 16.  8.  1.  0.]\n",
      " [ 0.  0.  0.  8. 10.  0.  0.  0.  0.  0.  3. 15.  5.  0.  0.  0.  0.  0.\n",
      "   7. 13.  0.  0.  0.  0.  0.  0.  7. 14.  5.  1.  0.  0.  0.  0.  6. 16.\n",
      "  16. 16.  3.  0.  0.  0.  6. 16.  7. 13.  8.  0.  0.  0.  2. 15.  7. 15.\n",
      "   7.  0.  0.  0.  0.  7. 15. 12.  0.  0.]\n",
      " [ 0.  0.  6. 14. 13.  3.  0.  0.  0.  0. 12.  2.  3. 14.  0.  0.  0.  0.\n",
      "   0.  0.  8. 13.  0.  0.  0.  0.  0. 12. 16.  3.  0.  0.  0.  0.  0.  0.\n",
      "   8. 13.  1.  0.  0.  1.  7.  0.  0.  7. 11.  0.  0.  3. 13.  2.  0.  7.\n",
      "  13.  0.  0.  0.  5. 14. 14. 15.  6.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all}\")\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 1414 rows;  testing with 354 rows\n",
      "\n",
      "Held-out data... (testing data: 354)\n",
      "y_test: [7. 0. 7. 6. 1. 3. 8. 3. 9. 0. 1. 9. 6. 8. 1. 6. 7. 5. 1. 8. 4. 3. 9. 0.\n",
      " 3. 5. 3. 0. 0. 3. 0. 9. 2. 8. 3. 5. 8. 4. 1. 1. 8. 7. 1. 3. 9. 0. 5. 3.\n",
      " 2. 1. 6. 2. 5. 4. 4. 5. 2. 0. 2. 1. 8. 7. 8. 3. 6. 1. 8. 8. 7. 2. 4. 4.\n",
      " 5. 7. 3. 5. 0. 9. 4. 7. 0. 9. 9. 9. 4. 4. 1. 6. 7. 3. 8. 3. 6. 8. 0. 5.\n",
      " 3. 7. 7. 4. 0. 2. 8. 6. 9. 0. 5. 3. 3. 5. 1. 9. 2. 6. 8. 8. 9. 9. 3. 8.\n",
      " 1. 5. 5. 3. 2. 2. 6. 8. 5. 4. 9. 5. 2. 0. 6. 8. 6. 7. 1. 5. 7. 3. 2. 7.\n",
      " 6. 8. 4. 4. 1. 2. 7. 0. 5. 2. 8. 1. 4. 5. 3. 8. 5. 6. 7. 0. 4. 3. 0. 4.\n",
      " 3. 8. 0. 2. 3. 9. 8. 8. 0. 2. 7. 4. 5. 0. 0. 3. 6. 8. 0. 4. 2. 5. 1. 0.\n",
      " 5. 1. 5. 6. 9. 2. 2. 3. 8. 8. 2. 0. 8. 2. 3. 5. 0. 0. 5. 0. 2. 5. 4. 8.\n",
      " 8. 5. 6. 6. 6. 4. 9. 3. 4. 6. 1. 1. 2. 8. 6. 5. 1. 5. 3. 8. 2. 7. 4. 8.\n",
      " 0. 2. 7. 6. 9. 8. 7. 7. 2. 4. 9. 7. 4. 6. 4. 8. 0. 0. 4. 1. 3. 6. 8. 6.\n",
      " 2. 8. 6. 0. 7. 1. 7. 0. 8. 1. 9. 9. 5. 5. 6. 8. 1. 5. 2. 1. 6. 8. 9. 1.\n",
      " 3. 3. 3. 5. 2. 1. 0. 6. 9. 1. 0. 4. 7. 0. 0. 6. 3. 1. 6. 8. 3. 7. 5. 9.\n",
      " 5. 7. 9. 9. 5. 2. 3. 6. 1. 8. 1. 4. 9. 5. 5. 3. 9. 3. 8. 7. 2. 6. 1. 6.\n",
      " 4. 0. 5. 5. 6. 6. 7. 4. 3. 7. 3. 8. 3. 3. 9. 6. 3. 7.]\n",
      "\n",
      "X_test (few rows): [[ 0.  0. 12. 16. 16.  5.  0.  0.  0.  3. 13.  8. 14. 15.  1.  0.  0.  0.\n",
      "   0.  0. 13. 16.  0.  0.  0.  6. 16. 16. 16. 16. 13.  0.  0.  6.  9. 11.\n",
      "  16.  9.  5.  0.  0.  0.  0. 14. 11.  0.  0.  0.  0.  0.  7. 16.  2.  0.\n",
      "   0.  0.  0.  0. 13. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  6. 14. 13.  1.  0.  0.  0.  3. 16. 10.  5. 11.  0.  0.  0.  5.\n",
      "  16.  0.  0. 13.  0.  0.  0.  6. 12.  0.  0. 12.  3.  0.  0.  7. 12.  0.\n",
      "   0. 13.  3.  0.  0.  3. 11.  0.  5. 12.  0.  0.  0.  0. 13.  4. 15.  4.\n",
      "   0.  0.  0.  0.  5. 16.  6.  0.  0.  0.]\n",
      " [ 0.  0.  5. 15. 16. 14.  1.  0.  0.  0. 11. 13.  9. 16.  5.  0.  0.  0.\n",
      "   0.  0.  5. 16.  2.  0.  0.  0.  0.  0.  9. 11.  0.  0.  0.  0.  7. 13.\n",
      "  15. 12.  1.  0.  0.  0.  7. 14. 14. 12.  4.  0.  0.  0.  0. 14.  3.  0.\n",
      "   0.  0.  0.  0.  7. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 14.  1.  0.  0.  0.  0.  5. 16. 12.  0.  0.  0.  0.  0.\n",
      "  10. 15.  1.  0.  0.  0.  0.  0. 14. 15.  9.  2.  0.  0.  0.  1. 16. 15.\n",
      "  16. 15.  2.  0.  0.  0. 15.  7.  1. 12. 10.  0.  0.  0. 10. 14.  4. 15.\n",
      "  12.  0.  0.  0.  0. 11. 16. 15.  5.  0.]\n",
      " [ 0.  0.  0.  3. 16.  8.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  3.\n",
      "  12. 16. 16.  8.  0.  0.  0. 13. 16.  9. 16.  8.  0.  0.  0.  1.  2.  0.\n",
      "  16.  8.  0.  0.  0.  0.  0.  2. 16.  4.  0.  0.  0.  0.  0.  2. 16.  6.\n",
      "   0.  0.  0.  0.  0.  1. 16.  9.  0.  0.]]\n",
      "\n",
      "Data used for modeling... (training data: 1414)\n",
      "y_train: [9. 5. 4. ... 8. 8. 4.]\n",
      "\n",
      "X_train (few rows): [[ 0.  0. 12. 16.  7.  0.  0.  0.  0.  2. 16.  5. 12.  3.  0.  0.  0.  0.\n",
      "  14.  6.  3. 16.  2.  0.  0.  0.  2. 14. 16. 12.  0.  0.  0.  0.  0.  0.\n",
      "  10. 10.  0.  0.  0.  0.  0.  0. 10.  8.  0.  0.  0.  0.  8.  2. 13.  7.\n",
      "   0.  0.  0.  0. 11. 16. 16.  3.  0.  0.]\n",
      " [ 0.  0. 12. 16. 16. 16.  7.  0.  0.  1. 14. 15.  6.  4.  1.  0.  0.  8.\n",
      "  16.  2.  0.  0.  0.  0.  0.  9. 16. 12. 12.  9.  1.  0.  0.  1.  8.  8.\n",
      "   8. 15. 10.  0.  0.  0.  0.  0.  0. 13. 12.  0.  0.  0.  8.  2.  6. 16.\n",
      "   5.  0.  0.  1. 11. 16. 16.  8.  0.  0.]\n",
      " [ 0.  0.  0.  3. 14.  9.  0.  0.  0.  0.  0. 13. 11.  1.  0.  0.  0.  0.\n",
      "   9. 14.  0.  0.  0.  0.  0.  4. 16.  4.  0.  4.  2.  0.  0. 12. 12.  7.\n",
      "  14. 16. 10.  0.  0. 13. 16. 14. 11. 16.  4.  0.  0.  2.  2.  0. 11. 13.\n",
      "   0.  0.  0.  0.  0.  3. 16.  9.  0.  0.]\n",
      " [ 0.  0.  6. 14. 15.  7.  0.  0.  0.  3. 15.  6.  2. 14.  3.  0.  0.  4.\n",
      "  13.  0.  1. 16.  4.  0.  0.  0. 10. 11.  9. 16.  6.  0.  0.  0.  1.  8.\n",
      "  10. 14.  5.  0.  0.  0.  0.  0.  0.  8. 11.  0.  0.  1. 12.  5.  0. 10.\n",
      "  11.  0.  0.  0.  7. 13. 16. 16.  4.  0.]\n",
      " [ 0.  0.  9. 10.  2.  0.  0.  0.  0.  8. 16. 16. 10.  0.  0.  0.  0.  7.\n",
      "   7.  4. 16.  2.  0.  0.  0.  0.  0.  8. 16.  5.  0.  0.  0.  0.  0. 10.\n",
      "  16. 14.  2.  0.  0.  0.  0.  0.  2. 14.  7.  0.  0.  0. 11. 10.  4. 11.\n",
      "  12.  0.  0.  0.  8. 14. 16. 15.  6.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "         [ 0.         -0.33004633  1.44429923  0.98933007] -> ?     9.00 \n",
      "         [ 0.         -0.33004633  1.44429923  0.98933007] -> ?     5.00 \n",
      "         [ 0.         -0.33004633 -1.07568734 -2.10560957] -> ?     4.00 \n",
      "         [ 0.         -0.33004633  0.18430595  0.51318551] -> ?     9.00 \n",
      "         [ 0.         -0.33004633  0.81430259 -0.43910361] -> ?     3.00 \n",
      "\n",
      "                                                    input  -> pred  des. \n",
      "                                         [ 0.  0. 12. 16.] -> ?     9.00 \n",
      "                                         [ 0.  0. 12. 16.] -> ?     5.00 \n",
      "                                             [0. 0. 0. 3.] -> ?     4.00 \n",
      "                                         [ 0.  0.  6. 14.] -> ?     9.00 \n",
      "                                         [ 0.  0.  9. 10.] -> ?     3.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's create a table for showing our data and its predictions...\n",
    "#\n",
    "def ascii_table(X,y,scaler_to_invert=None):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    if scaler_to_invert == None:  # don't use the scaler\n",
    "        X = X\n",
    "    else:\n",
    "        X = scaler_to_invert.inverse_transform(X)\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        # whoa! serious f-string formatting:\n",
    "        print(f\"{X[i,0:4]!s:>58s} -> {'?':<5s} {y[i]:<5.2f}\")   # !s is str ...\n",
    "    print()\n",
    "    \n",
    "# to show the table with the scaled data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# to show the table with the original data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 2.14309482\n",
      "Iteration 2, loss = 1.59074616\n",
      "Iteration 3, loss = 1.19685674\n",
      "Iteration 4, loss = 0.90901111\n",
      "Iteration 5, loss = 0.72473294\n",
      "Iteration 6, loss = 0.61815391\n",
      "Iteration 7, loss = 0.53724776\n",
      "Iteration 8, loss = 0.47671793\n",
      "Iteration 9, loss = 0.42483472\n",
      "Iteration 10, loss = 0.39271809\n",
      "Iteration 11, loss = 0.34438686\n",
      "Iteration 12, loss = 0.30986025\n",
      "Iteration 13, loss = 0.28353243\n",
      "Iteration 14, loss = 0.29653293\n",
      "Iteration 15, loss = 0.27336638\n",
      "Iteration 16, loss = 0.23978764\n",
      "Iteration 17, loss = 0.23950766\n",
      "Iteration 18, loss = 0.23866846\n",
      "Iteration 19, loss = 0.20800636\n",
      "Iteration 20, loss = 0.18446410\n",
      "Iteration 21, loss = 0.20791722\n",
      "Iteration 22, loss = 0.18466908\n",
      "Iteration 23, loss = 0.17335212\n",
      "Iteration 24, loss = 0.16998188\n",
      "Iteration 25, loss = 0.15744200\n",
      "Iteration 26, loss = 0.14413024\n",
      "Iteration 27, loss = 0.13526985\n",
      "Iteration 28, loss = 0.15361941\n",
      "Iteration 29, loss = 0.13368437\n",
      "Iteration 30, loss = 0.12404986\n",
      "Iteration 31, loss = 0.13593062\n",
      "Iteration 32, loss = 0.12221152\n",
      "Iteration 33, loss = 0.11070265\n",
      "Iteration 34, loss = 0.09973405\n",
      "Iteration 35, loss = 0.09809462\n",
      "Iteration 36, loss = 0.09403143\n",
      "Iteration 37, loss = 0.13737880\n",
      "Iteration 38, loss = 0.25770914\n",
      "Iteration 39, loss = 0.20243626\n",
      "Iteration 40, loss = 0.15338407\n",
      "Iteration 41, loss = 0.13548635\n",
      "Iteration 42, loss = 0.12016516\n",
      "Iteration 43, loss = 0.10969389\n",
      "Iteration 44, loss = 0.10414104\n",
      "Iteration 45, loss = 0.09779579\n",
      "Iteration 46, loss = 0.09364291\n",
      "Iteration 47, loss = 0.09958979\n",
      "Iteration 48, loss = 0.10140475\n",
      "Iteration 49, loss = 0.09426563\n",
      "Iteration 50, loss = 0.08645912\n",
      "Iteration 51, loss = 0.09365111\n",
      "Iteration 52, loss = 0.08602499\n",
      "Iteration 53, loss = 0.08144000\n",
      "Iteration 54, loss = 0.07681558\n",
      "Iteration 55, loss = 0.07284579\n",
      "Iteration 56, loss = 0.07280622\n",
      "Iteration 57, loss = 0.07523888\n",
      "Iteration 58, loss = 0.07280087\n",
      "Iteration 59, loss = 0.06758234\n",
      "Iteration 60, loss = 0.07072588\n",
      "Iteration 61, loss = 0.07399945\n",
      "Iteration 62, loss = 0.06796109\n",
      "Iteration 63, loss = 0.06413603\n",
      "Iteration 64, loss = 0.06268720\n",
      "Iteration 65, loss = 0.06122522\n",
      "Iteration 66, loss = 0.06098965\n",
      "Iteration 67, loss = 0.05783740\n",
      "Iteration 68, loss = 0.05732830\n",
      "Iteration 69, loss = 0.05653411\n",
      "Iteration 70, loss = 0.05785761\n",
      "Iteration 71, loss = 0.05604445\n",
      "Iteration 72, loss = 0.05544920\n",
      "Iteration 73, loss = 0.06811674\n",
      "Iteration 74, loss = 0.08065660\n",
      "Iteration 75, loss = 0.08293006\n",
      "Iteration 76, loss = 0.06689757\n",
      "Iteration 77, loss = 0.05882983\n",
      "Iteration 78, loss = 0.05491017\n",
      "Iteration 79, loss = 0.06489640\n",
      "Iteration 80, loss = 0.07010314\n",
      "Iteration 81, loss = 0.12202865\n",
      "Iteration 82, loss = 0.11814220\n",
      "Iteration 83, loss = 0.08380247\n",
      "Iteration 84, loss = 0.08764094\n",
      "Iteration 85, loss = 0.09245459\n",
      "Iteration 86, loss = 0.08980108\n",
      "Iteration 87, loss = 0.07160232\n",
      "Iteration 88, loss = 0.06292316\n",
      "Iteration 89, loss = 0.05582036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 90, loss = 0.05658000\n",
      "Iteration 91, loss = 0.05171615\n",
      "Iteration 92, loss = 0.05046834\n",
      "Iteration 93, loss = 0.04980662\n",
      "Iteration 94, loss = 0.04929729\n",
      "Iteration 95, loss = 0.04871172\n",
      "Iteration 96, loss = 0.04826916\n",
      "Iteration 97, loss = 0.04785028\n",
      "Iteration 98, loss = 0.04759707\n",
      "Iteration 99, loss = 0.04731155\n",
      "Iteration 100, loss = 0.04701393\n",
      "Iteration 101, loss = 0.04673943\n",
      "Iteration 102, loss = 0.04649678\n",
      "Iteration 103, loss = 0.04627560\n",
      "Iteration 104, loss = 0.04603568\n",
      "Iteration 105, loss = 0.04578146\n",
      "Iteration 106, loss = 0.04560741\n",
      "Iteration 107, loss = 0.04541986\n",
      "Iteration 108, loss = 0.04520597\n",
      "Iteration 109, loss = 0.04497951\n",
      "Iteration 110, loss = 0.04490047\n",
      "Iteration 111, loss = 0.04478221\n",
      "Iteration 112, loss = 0.04448365\n",
      "Iteration 113, loss = 0.04415331\n",
      "Iteration 114, loss = 0.04392684\n",
      "Iteration 115, loss = 0.04380598\n",
      "Iteration 116, loss = 0.04370284\n",
      "Iteration 117, loss = 0.04352703\n",
      "Iteration 118, loss = 0.04328688\n",
      "Iteration 119, loss = 0.04306842\n",
      "Iteration 120, loss = 0.04289477\n",
      "Iteration 121, loss = 0.04281084\n",
      "Iteration 122, loss = 0.04261864\n",
      "Iteration 123, loss = 0.04244265\n",
      "Iteration 124, loss = 0.04227121\n",
      "Iteration 125, loss = 0.04213812\n",
      "Iteration 126, loss = 0.04212685\n",
      "Iteration 127, loss = 0.04203598\n",
      "Iteration 128, loss = 0.04191113\n",
      "Iteration 129, loss = 0.04174188\n",
      "Iteration 130, loss = 0.04174936\n",
      "Iteration 131, loss = 0.04150751\n",
      "Iteration 132, loss = 0.04134354\n",
      "Iteration 133, loss = 0.04134593\n",
      "Iteration 134, loss = 0.04127430\n",
      "Iteration 135, loss = 0.04119592\n",
      "Iteration 136, loss = 0.04121580\n",
      "Iteration 137, loss = 0.04127977\n",
      "Iteration 138, loss = 0.04123274\n",
      "Iteration 139, loss = 0.04101037\n",
      "Iteration 140, loss = 0.04055562\n",
      "Iteration 141, loss = 0.04060829\n",
      "Iteration 142, loss = 0.04033583\n",
      "Iteration 143, loss = 0.04021072\n",
      "Iteration 144, loss = 0.04009407\n",
      "Iteration 145, loss = 0.04000032\n",
      "Iteration 146, loss = 0.03985836\n",
      "Iteration 147, loss = 0.03979568\n",
      "Iteration 148, loss = 0.03976172\n",
      "Iteration 149, loss = 0.03962947\n",
      "Iteration 150, loss = 0.03955246\n",
      "Iteration 151, loss = 0.03952095\n",
      "Iteration 152, loss = 0.03952746\n",
      "Iteration 153, loss = 0.03965684\n",
      "Iteration 154, loss = 0.03988826\n",
      "Iteration 155, loss = 0.03990760\n",
      "Iteration 156, loss = 0.03957529\n",
      "Iteration 157, loss = 0.03935801\n",
      "Iteration 158, loss = 0.03916776\n",
      "Iteration 159, loss = 0.03896454\n",
      "Iteration 160, loss = 0.03887207\n",
      "Iteration 161, loss = 0.03872816\n",
      "Iteration 162, loss = 0.03853869\n",
      "Iteration 163, loss = 0.03841286\n",
      "Iteration 164, loss = 0.03831722\n",
      "Iteration 165, loss = 0.03819607\n",
      "Iteration 166, loss = 0.03839409\n",
      "Iteration 167, loss = 0.03834662\n",
      "Iteration 168, loss = 0.03810014\n",
      "Iteration 169, loss = 0.03787357\n",
      "Iteration 170, loss = 0.03769079\n",
      "Iteration 171, loss = 0.03762246\n",
      "Iteration 172, loss = 0.03752136\n",
      "Iteration 173, loss = 0.03742995\n",
      "Iteration 174, loss = 0.03750058\n",
      "Iteration 175, loss = 0.03762781\n",
      "Iteration 176, loss = 0.03747137\n",
      "Iteration 177, loss = 0.03733900\n",
      "Iteration 178, loss = 0.03718960\n",
      "Iteration 179, loss = 0.03712776\n",
      "Iteration 180, loss = 0.03706670\n",
      "Iteration 181, loss = 0.03689855\n",
      "Iteration 182, loss = 0.03672742\n",
      "Iteration 183, loss = 0.03664109\n",
      "Iteration 184, loss = 0.03659491\n",
      "Iteration 185, loss = 0.03646624\n",
      "Iteration 186, loss = 0.03635880\n",
      "Iteration 187, loss = 0.03622547\n",
      "Iteration 188, loss = 0.03629746\n",
      "Iteration 189, loss = 0.03618315\n",
      "Iteration 190, loss = 0.03598711\n",
      "Iteration 191, loss = 0.03606058\n",
      "Iteration 192, loss = 0.03601549\n",
      "Iteration 193, loss = 0.03586673\n",
      "Iteration 194, loss = 0.03578045\n",
      "Iteration 195, loss = 0.03557811\n",
      "Iteration 196, loss = 0.03544641\n",
      "Iteration 197, loss = 0.03527851\n",
      "Iteration 198, loss = 0.03524138\n",
      "Iteration 199, loss = 0.03509686\n",
      "Iteration 200, loss = 0.03496902\n",
      "Iteration 201, loss = 0.03486448\n",
      "Iteration 202, loss = 0.03476251\n",
      "Iteration 203, loss = 0.03480587\n",
      "Iteration 204, loss = 0.03475572\n",
      "Iteration 205, loss = 0.03465401\n",
      "Iteration 206, loss = 0.03450868\n",
      "Iteration 207, loss = 0.03442862\n",
      "Iteration 208, loss = 0.03431459\n",
      "Iteration 209, loss = 0.03432434\n",
      "Iteration 210, loss = 0.03416195\n",
      "Iteration 211, loss = 0.03411909\n",
      "Iteration 212, loss = 0.03412147\n",
      "Iteration 213, loss = 0.03403214\n",
      "Iteration 214, loss = 0.03385716\n",
      "Iteration 215, loss = 0.03373713\n",
      "Iteration 216, loss = 0.03359366\n",
      "Iteration 217, loss = 0.03351266\n",
      "Iteration 218, loss = 0.03345020\n",
      "Iteration 219, loss = 0.03338428\n",
      "Iteration 220, loss = 0.03334488\n",
      "Iteration 221, loss = 0.03328344\n",
      "Iteration 222, loss = 0.03319241\n",
      "Iteration 223, loss = 0.03310911\n",
      "Iteration 224, loss = 0.03309003\n",
      "Iteration 225, loss = 0.03331299\n",
      "Iteration 226, loss = 0.03320056\n",
      "Iteration 227, loss = 0.03297390\n",
      "Iteration 228, loss = 0.03299785\n",
      "Iteration 229, loss = 0.03286783\n",
      "Iteration 230, loss = 0.03273302\n",
      "Iteration 231, loss = 0.03264749\n",
      "Iteration 232, loss = 0.03265326\n",
      "Iteration 233, loss = 0.03259260\n",
      "Iteration 234, loss = 0.03246201\n",
      "Iteration 235, loss = 0.03235594\n",
      "Iteration 236, loss = 0.03233454\n",
      "Iteration 237, loss = 0.03226179\n",
      "Iteration 238, loss = 0.03216679\n",
      "Iteration 239, loss = 0.03211749\n",
      "Iteration 240, loss = 0.03205674\n",
      "Iteration 241, loss = 0.03193078\n",
      "Iteration 242, loss = 0.03210315\n",
      "Iteration 243, loss = 0.03220223\n",
      "Iteration 244, loss = 0.03205313\n",
      "Iteration 245, loss = 0.03183516\n",
      "Iteration 246, loss = 0.03167708\n",
      "Iteration 247, loss = 0.03169397\n",
      "Iteration 248, loss = 0.03168977\n",
      "Iteration 249, loss = 0.03167998\n",
      "Iteration 250, loss = 0.03173022\n",
      "Iteration 251, loss = 0.03158941\n",
      "Iteration 252, loss = 0.03142454\n",
      "Iteration 253, loss = 0.03135654\n",
      "Iteration 254, loss = 0.03125537\n",
      "Iteration 255, loss = 0.03118032\n",
      "Iteration 256, loss = 0.03106432\n",
      "Iteration 257, loss = 0.03100551\n",
      "Iteration 258, loss = 0.03094367\n",
      "Iteration 259, loss = 0.03087925\n",
      "Iteration 260, loss = 0.03083465\n",
      "Iteration 261, loss = 0.03085439\n",
      "Iteration 262, loss = 0.03074884\n",
      "Iteration 263, loss = 0.03069298\n",
      "Iteration 264, loss = 0.03064102\n",
      "Iteration 265, loss = 0.03060384\n",
      "Iteration 266, loss = 0.03063526\n",
      "Iteration 267, loss = 0.03057594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 268, loss = 0.03045882\n",
      "Iteration 269, loss = 0.03047730\n",
      "Iteration 270, loss = 0.03046363\n",
      "Iteration 271, loss = 0.03042954\n",
      "Iteration 272, loss = 0.03040192\n",
      "Iteration 273, loss = 0.03035919\n",
      "Iteration 274, loss = 0.03032151\n",
      "Iteration 275, loss = 0.03029826\n",
      "Iteration 276, loss = 0.03025166\n",
      "Iteration 277, loss = 0.03024563\n",
      "Iteration 278, loss = 0.03021177\n",
      "Iteration 279, loss = 0.03020216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 280, loss = 0.03016861\n",
      "Iteration 281, loss = 0.03015550\n",
      "Iteration 282, loss = 0.03014360\n",
      "Iteration 283, loss = 0.03013669\n",
      "Iteration 284, loss = 0.03012614\n",
      "Iteration 285, loss = 0.03012205\n",
      "Iteration 286, loss = 0.03011685\n",
      "Iteration 287, loss = 0.03011297\n",
      "Iteration 288, loss = 0.03010749\n",
      "Iteration 289, loss = 0.03010174\n",
      "Iteration 290, loss = 0.03009881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 291, loss = 0.03008445\n",
      "Iteration 292, loss = 0.03007692\n",
      "Iteration 293, loss = 0.03007671\n",
      "Iteration 294, loss = 0.03007586\n",
      "Iteration 295, loss = 0.03007462\n",
      "Iteration 296, loss = 0.03007373\n",
      "Iteration 297, loss = 0.03007313\n",
      "Iteration 298, loss = 0.03007220\n",
      "Iteration 299, loss = 0.03007138\n",
      "Iteration 300, loss = 0.03007050\n",
      "Iteration 301, loss = 0.03006966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 302, loss = 0.03006823\n",
      "Iteration 303, loss = 0.03006782\n",
      "Iteration 304, loss = 0.03006760\n",
      "Iteration 305, loss = 0.03006735\n",
      "Iteration 306, loss = 0.03006725\n",
      "Iteration 307, loss = 0.03006709\n",
      "Iteration 308, loss = 0.03006659\n",
      "Iteration 309, loss = 0.03006636\n",
      "Iteration 310, loss = 0.03006618\n",
      "Iteration 311, loss = 0.03006602\n",
      "Iteration 312, loss = 0.03006587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 313, loss = 0.03006555\n",
      "Iteration 314, loss = 0.03006543\n",
      "Iteration 315, loss = 0.03006537\n",
      "Iteration 316, loss = 0.03006532\n",
      "Iteration 317, loss = 0.03006528\n",
      "Iteration 318, loss = 0.03006526\n",
      "Iteration 319, loss = 0.03006521\n",
      "Iteration 320, loss = 0.03006518\n",
      "Iteration 321, loss = 0.03006516\n",
      "Iteration 322, loss = 0.03006511\n",
      "Iteration 323, loss = 0.03006509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 324, loss = 0.03006502\n",
      "Iteration 325, loss = 0.03006501\n",
      "Iteration 326, loss = 0.03006500\n",
      "Iteration 327, loss = 0.03006499\n",
      "Iteration 328, loss = 0.03006498\n",
      "Iteration 329, loss = 0.03006497\n",
      "Iteration 330, loss = 0.03006496\n",
      "Iteration 331, loss = 0.03006496\n",
      "Iteration 332, loss = 0.03006495\n",
      "Iteration 333, loss = 0.03006494\n",
      "Iteration 334, loss = 0.03006493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 335, loss = 0.03006492\n",
      "Iteration 336, loss = 0.03006492\n",
      "Iteration 337, loss = 0.03006492\n",
      "Iteration 338, loss = 0.03006492\n",
      "Iteration 339, loss = 0.03006491\n",
      "Iteration 340, loss = 0.03006491\n",
      "Iteration 341, loss = 0.03006491\n",
      "Iteration 342, loss = 0.03006491\n",
      "Iteration 343, loss = 0.03006491\n",
      "Iteration 344, loss = 0.03006491\n",
      "Iteration 345, loss = 0.03006490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.030064904918854093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->  pred   des. \n",
      "           [ 0.  0. 12. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  6. 14.] ->   0      0      correct \n",
      "           [ 0.  0.  5. 15.] ->   7      7      correct \n",
      "           [ 0.  0.  0. 12.] ->   6      6      correct \n",
      "               [0. 0. 0. 3.] ->   1      1      correct \n",
      "           [ 0.  2. 13. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  4. 15.] ->   8      8      correct \n",
      "           [ 0.  2. 11. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  7. 14.] ->   9      9      correct \n",
      "           [ 0.  0.  3. 12.] ->   0      0      correct \n",
      "           [ 0.  0. 13. 14.] ->   2      1      incorrect: [6.76196833e-03 1.01187252e-03 8.17538439e-01 4.46415240e-04\n",
      " 1.01785457e-04 9.76132704e-05 4.60634227e-04 4.08457673e-04\n",
      " 1.73160474e-01 1.23406617e-05]\n",
      "           [ 0.  0.  2. 12.] ->   9      9      correct \n",
      "           [ 0.  0.  3. 11.] ->   6      6      correct \n",
      "               [0. 0. 1. 6.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 15.] ->   8      1      incorrect: [4.36531388e-07 3.16766964e-01 9.32930286e-04 1.72636088e-03\n",
      " 2.54272303e-03 9.86340334e-03 8.33165263e-03 1.00600978e-03\n",
      " 6.58755056e-01 7.44643042e-05]\n",
      "               [0. 0. 0. 7.] ->   6      6      correct \n",
      "           [ 0.  0. 10. 16.] ->   7      7      correct \n",
      "           [ 0.  0. 11. 14.] ->   5      5      correct \n",
      "               [0. 0. 0. 4.] ->   1      1      correct \n",
      "           [ 0.  3.  5. 14.] ->   8      8      correct \n",
      "           [ 0.  0.  0. 11.] ->   4      4      correct \n",
      "           [ 0.  0. 11. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  7. 16.] ->   8      9      incorrect: [3.65571795e-08 9.54270855e-03 4.69764957e-06 3.37217507e-02\n",
      " 1.04631240e-07 3.16668565e-04 1.63750470e-05 3.22861856e-06\n",
      " 9.56092029e-01 3.02400168e-04]\n",
      "           [ 0.  0.  6. 16.] ->   0      0      correct \n",
      "           [ 0.  1. 10. 15.] ->   3      3      correct \n",
      "           [ 0.  3. 14. 14.] ->   5      5      correct \n",
      "           [ 0.  0. 11. 15.] ->   2      3      incorrect: [1.37749633e-06 6.05122046e-06 9.46443666e-01 5.30835835e-02\n",
      " 1.54866411e-07 1.40946220e-05 2.84426760e-09 5.88112380e-05\n",
      " 4.72591083e-05 3.44999156e-04]\n",
      "           [ 0.  0.  1. 15.] ->   0      0      correct \n",
      "           [ 0.  0.  0. 10.] ->   0      0      correct \n",
      "           [ 0.  0.  5. 11.] ->   3      3      correct \n",
      "           [ 0.  0.  2. 10.] ->   0      0      correct \n",
      "           [ 0.  0.  8. 16.] ->   9      9      correct \n",
      "           [ 0.  2. 11. 16.] ->   2      2      correct \n",
      "           [ 0.  3. 16.  9.] ->   8      8      correct \n",
      "           [ 0.  0.  6. 14.] ->   2      3      incorrect: [3.33323121e-02 1.16746830e-05 9.32403432e-01 1.35135800e-04\n",
      " 1.14497427e-02 1.93927433e-02 1.16968692e-06 2.41235066e-03\n",
      " 1.57374882e-07 8.61282164e-04]\n",
      "           [ 0.  0.  9. 16.] ->   2      5      incorrect: [4.03052294e-03 9.60532308e-07 6.32122991e-01 1.32140689e-03\n",
      " 1.18108101e-03 1.97563232e-03 9.39816991e-08 3.07323913e-01\n",
      " 6.85012272e-06 5.20365482e-02]\n",
      "               [0. 0. 0. 6.] ->   8      8      correct \n",
      "               [0. 0. 0. 1.] ->   4      4      correct \n",
      "               [0. 0. 0. 0.] ->   1      1      correct \n",
      "           [ 0.  0.  1. 12.] ->   1      1      correct \n",
      "           [ 0.  0.  1. 14.] ->   8      8      correct \n",
      "           [ 0.  0.  2. 13.] ->   7      7      correct \n",
      "           [ 0.  0.  0. 15.] ->   1      1      correct \n",
      "           [ 0.  0. 10. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  9. 13.] ->   9      9      correct \n",
      "           [ 0.  0.  2. 12.] ->   0      0      correct \n",
      "           [ 0.  1.  9. 12.] ->   5      5      correct \n",
      "           [ 0.  0.  7. 15.] ->   3      3      correct \n",
      "               [0. 0. 0. 5.] ->   2      2      correct \n",
      "               [0. 0. 0. 0.] ->   1      1      correct \n",
      "           [ 0.  0.  4. 14.] ->   6      6      correct \n",
      "           [ 0.  3. 15. 14.] ->   2      2      correct \n",
      "           [ 0.  5. 16. 16.] ->   5      5      correct \n",
      "               [0. 0. 0. 9.] ->   4      4      correct \n",
      "               [0. 0. 0. 7.] ->   4      4      correct \n",
      "           [ 0.  4.  7. 13.] ->   2      5      incorrect: [3.98960228e-04 5.54837187e-04 9.89833324e-01 2.65343734e-04\n",
      " 5.00210288e-05 8.86390287e-03 9.79040259e-06 6.10505329e-07\n",
      " 2.31048404e-05 1.04804709e-07]\n",
      "           [ 0.  4. 16. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 11.] ->   0      0      correct \n",
      "           [ 0.  1. 13. 16.] ->   2      2      correct \n",
      "               [0. 0. 0. 1.] ->   1      1      correct \n",
      "           [ 0.  0.  2. 13.] ->   8      8      correct \n",
      "           [ 0.  0.  8. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  5. 14.] ->   8      8      correct \n",
      "           [ 0.  1. 12. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  3. 11.] ->   6      6      correct \n",
      "           [ 0.  0.  0. 16.] ->   1      1      correct \n",
      "           [ 0.  0.  3. 13.] ->   8      8      correct \n",
      "               [0. 0. 0. 8.] ->   1      8      incorrect: [1.48779408e-05 9.45062932e-01 4.27597171e-05 5.36232566e-06\n",
      " 1.98917347e-04 3.94735896e-07 3.09810108e-03 6.23965606e-08\n",
      " 5.15532367e-02 2.33559085e-05]\n",
      "           [ 0.  0.  3. 16.] ->   7      7      correct \n",
      "           [ 0.  4. 16. 15.] ->   2      2      correct \n",
      "               [0. 0. 0. 5.] ->   4      4      correct \n",
      "               [0. 0. 0. 3.] ->   4      4      correct \n",
      "           [ 0.  1.  8. 12.] ->   5      5      correct \n",
      "           [ 0.  0.  8. 16.] ->   9      7      incorrect: [1.28628272e-06 6.50989642e-04 7.13392721e-06 3.86968117e-04\n",
      " 7.76969685e-03 8.28439438e-04 7.84639821e-05 4.78181222e-01\n",
      " 2.73337443e-03 5.09362426e-01]\n",
      "           [ 0.  1.  8. 12.] ->   3      3      correct \n",
      "           [ 0.  5. 12. 13.] ->   5      5      correct \n",
      "           [ 0.  0.  6. 15.] ->   0      0      correct \n",
      "           [ 0.  0. 11. 12.] ->   9      9      correct \n",
      "               [0. 0. 0. 8.] ->   4      4      correct \n",
      "           [ 0.  0.  6. 12.] ->   7      7      correct \n",
      "           [ 0.  0. 10. 11.] ->   0      0      correct \n",
      "           [ 0.  0. 10. 15.] ->   9      9      correct \n",
      "           [ 0.  0.  7. 16.] ->   9      9      correct \n",
      "           [ 0.  0.  6. 14.] ->   9      9      correct \n",
      "               [0. 0. 0. 3.] ->   4      4      correct \n",
      "               [0. 0. 0. 5.] ->   4      4      correct \n",
      "           [ 0.  0.  4. 10.] ->   1      1      correct \n",
      "           [ 0.  0.  2. 15.] ->   6      6      correct \n",
      "           [ 0.  4. 15. 16.] ->   7      7      correct \n",
      "               [0. 0. 1. 9.] ->   3      3      correct \n",
      "           [ 0.  0.  7. 15.] ->   8      8      correct \n",
      "           [ 0.  0.  7. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  7. 15.] ->   4      6      incorrect: [5.87665143e-02 1.25450062e-01 1.23308916e-05 3.52791204e-08\n",
      " 7.15248220e-01 6.70469444e-06 9.59723721e-02 4.93698414e-05\n",
      " 3.01758213e-03 1.47680876e-03]\n",
      "               [0. 0. 0. 3.] ->   8      8      correct \n",
      "           [ 0.  0.  2. 12.] ->   0      0      correct \n",
      "           [ 0.  4. 16. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  4. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  4. 11.] ->   9      7      incorrect: [1.43453897e-06 9.87375114e-04 7.39797357e-06 4.13281887e-04\n",
      " 9.41368194e-03 1.09329847e-03 9.02269641e-05 4.14389893e-01\n",
      " 4.01770999e-03 5.69585701e-01]\n",
      "           [ 0.  0.  3. 15.] ->   7      7      correct \n",
      "           [ 0.  0.  0. 13.] ->   4      4      correct \n",
      "               [0. 0. 1. 9.] ->   0      0      correct \n",
      "           [ 0.  0.  7. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  2. 13.] ->   8      8      correct \n",
      "               [0. 0. 0. 6.] ->   6      6      correct \n",
      "               [0. 0. 0. 0.] ->   8      9      incorrect: [5.29473335e-09 8.33219835e-02 7.23468894e-07 2.94683192e-03\n",
      " 6.38561700e-07 4.14079660e-05 4.92004115e-05 9.18445043e-07\n",
      " 9.13398871e-01 2.39419006e-04]\n",
      "           [ 0.  0.  4. 16.] ->   0      0      correct \n",
      "               [0. 0. 5. 8.] ->   5      5      correct \n",
      "           [ 0.  0.  5. 15.] ->   3      3      correct \n",
      "           [ 0.  0.  9. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  8. 15.] ->   5      5      correct \n",
      "           [ 0.  0.  2. 16.] ->   5      1      incorrect: [1.09441364e-04 2.95310595e-02 4.83831289e-04 1.40289694e-01\n",
      " 2.14015752e-06 8.11710200e-01 1.67028059e-03 6.76738094e-07\n",
      " 1.61956612e-02 7.01506797e-06]\n",
      "           [ 0.  0.  6. 16.] ->   9      9      correct \n",
      "           [ 0.  1. 15. 16.] ->   2      2      correct \n",
      "               [0. 0. 8. 7.] ->   6      6      correct \n",
      "           [ 0.  0. 11. 16.] ->   3      8      incorrect: [2.74817583e-08 2.36397778e-06 1.52557669e-04 9.98779969e-01\n",
      " 7.10484796e-10 1.21266182e-04 2.45237050e-08 3.82795729e-05\n",
      " 5.40650437e-04 3.64860359e-04]\n",
      "           [ 0.  0. 13. 16.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 13.] ->   9      9      correct \n",
      "               [0. 0. 0. 0.] ->   8      9      incorrect: [1.67851219e-05 6.06121975e-04 3.10950977e-09 1.99737755e-06\n",
      " 2.29537102e-04 4.67930398e-06 7.81033399e-03 9.34676665e-03\n",
      " 9.64997342e-01 1.69864337e-02]\n",
      "           [ 0.  1. 10. 16.] ->   5      3      incorrect: [1.07759616e-04 1.00553233e-04 1.42619653e-03 5.10958949e-04\n",
      " 4.52372707e-05 9.97435713e-01 3.13036517e-04 2.64955919e-05\n",
      " 3.38811600e-05 1.67903172e-07]\n",
      "           [ 0.  0.  1. 12.] ->   8      8      correct \n",
      "               [0. 0. 0. 7.] ->   1      1      correct \n",
      "           [ 0.  0.  7. 16.] ->   5      5      correct \n",
      "           [ 0.  0. 13. 10.] ->   5      5      correct \n",
      "           [ 0.  1.  5. 12.] ->   3      3      correct \n",
      "           [ 0.  3. 15. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  2. 16.] ->   6      6      correct \n",
      "           [ 0.  0.  1. 11.] ->   8      8      correct \n",
      "           [ 0.  0.  9. 12.] ->   8      5      incorrect: [9.64474970e-07 1.51191322e-03 7.20358181e-05 2.00672331e-01\n",
      " 5.27590046e-08 9.69541176e-03 1.70700081e-05 1.01616328e-05\n",
      " 7.87918483e-01 1.01575921e-04]\n",
      "               [0. 0. 0. 1.] ->   4      4      correct \n",
      "           [ 0.  0.  5. 11.] ->   7      9      incorrect: [6.14389786e-03 4.29556175e-07 4.81597903e-05 3.41806213e-03\n",
      " 2.13264351e-06 9.41571036e-06 1.15555176e-05 4.93344023e-01\n",
      " 1.31708531e-02 4.83851470e-01]\n",
      "           [ 0.  0. 11. 12.] ->   5      5      correct \n",
      "           [ 0.  0.  9. 11.] ->   2      2      correct \n",
      "               [0. 0. 7. 8.] ->   0      0      correct \n",
      "               [0. 0. 0. 6.] ->   6      6      correct \n",
      "           [ 0.  0.  4. 14.] ->   8      8      correct \n",
      "               [0. 0. 0. 8.] ->   6      6      correct \n",
      "           [ 0.  0.  8. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  0. 13.] ->   1      1      correct \n",
      "           [ 0.  2. 12. 13.] ->   5      5      correct \n",
      "           [ 0.  1. 11. 16.] ->   7      7      correct \n",
      "           [ 0.  2. 10. 12.] ->   3      3      correct \n",
      "           [ 0.  2. 13. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  0. 15.] ->   7      7      correct \n",
      "           [ 0.  0.  2. 15.] ->   6      6      correct \n",
      "               [0. 0. 0. 7.] ->   8      8      correct \n",
      "               [0. 0. 0. 1.] ->   4      4      correct \n",
      "               [0. 0. 0. 7.] ->   4      4      correct \n",
      "           [ 0.  0.  4. 14.] ->   1      1      correct \n",
      "               [0. 0. 0. 1.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  5. 13.] ->   0      0      correct \n",
      "           [ 0.  1. 12. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  6. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 14.] ->   8      8      correct \n",
      "               [0. 0. 0. 0.] ->   1      1      correct \n",
      "           [ 0.  0.  3. 15.] ->   1      4      incorrect: [3.89166199e-04 5.07390708e-01 7.17509897e-05 1.68042244e-05\n",
      " 2.26482902e-01 2.48190364e-04 3.06228251e-04 8.16424193e-05\n",
      " 2.33159997e-03 2.62681008e-01]\n",
      "               [0. 0. 4. 9.] ->   5      5      correct \n",
      "           [ 0.  1.  9. 15.] ->   3      3      correct \n",
      "           [ 0.  0. 13. 16.] ->   8      8      correct \n",
      "           [ 0.  0.  3. 14.] ->   5      5      correct \n",
      "           [ 0.  0.  1. 13.] ->   6      6      correct \n",
      "           [ 0.  0.  8. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  1. 12.] ->   0      0      correct \n",
      "               [0. 0. 0. 0.] ->   4      4      correct \n",
      "           [ 0.  0.  4. 13.] ->   3      3      correct \n",
      "           [ 0.  0. 11.  8.] ->   0      0      correct \n",
      "           [ 0.  0.  1. 14.] ->   4      4      correct \n",
      "           [ 0.  1. 12. 12.] ->   3      3      correct \n",
      "           [ 0.  0.  2. 12.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 15.] ->   0      0      correct \n",
      "           [ 0.  0. 13. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  9. 16.] ->   9      9      correct \n",
      "           [ 0.  0. 10. 14.] ->   8      8      correct \n",
      "           [ 0.  0. 10. 15.] ->   8      8      correct \n",
      "           [ 0.  0. 10. 16.] ->   0      0      correct \n",
      "           [ 0.  0. 11. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  3. 13.] ->   9      7      incorrect: [1.33683335e-06 6.21328198e-04 1.15957673e-05 5.61442641e-04\n",
      " 8.09146871e-03 1.47407686e-03 5.41318653e-05 4.58434741e-01\n",
      " 1.85771267e-03 5.28892165e-01]\n",
      "               [0. 0. 0. 8.] ->   4      4      correct \n",
      "           [ 0.  0.  6. 11.] ->   5      5      correct \n",
      "           [ 0.  0.  1. 12.] ->   0      0      correct \n",
      "           [ 0.  0.  2. 15.] ->   0      0      correct \n",
      "           [ 0.  0. 10. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  1. 13.] ->   6      6      correct \n",
      "           [ 0.  0.  3. 10.] ->   8      8      correct \n",
      "           [ 0.  0.  4. 12.] ->   0      0      correct \n",
      "               [0. 0. 0. 9.] ->   4      4      correct \n",
      "           [ 0.  0.  6. 15.] ->   2      2      correct \n",
      "           [ 0.  0.  7. 11.] ->   5      5      correct \n",
      "           [ 0.  0.  5. 12.] ->   1      1      correct \n",
      "           [ 0.  0.  9. 15.] ->   0      0      correct \n",
      "           [ 0.  4. 16. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  0. 14.] ->   1      1      correct \n",
      "           [ 0.  0.  7. 12.] ->   5      5      correct \n",
      "               [0. 0. 0. 6.] ->   6      6      correct \n",
      "           [ 0.  0.  5. 12.] ->   9      9      correct \n",
      "           [ 0.  1. 15. 15.] ->   2      2      correct \n",
      "           [ 0.  0. 10. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  3. 10.] ->   3      3      correct \n",
      "           [ 0.  1. 10.  9.] ->   8      8      correct \n",
      "           [ 0.  0.  0. 10.] ->   8      8      correct \n",
      "           [ 0.  3. 13. 16.] ->   2      2      correct \n",
      "               [0. 0. 9. 9.] ->   0      0      correct \n",
      "           [ 0.  0.  2. 11.] ->   8      8      correct \n",
      "           [ 0.  0. 11. 16.] ->   2      2      correct \n",
      "           [ 0.  0. 10. 16.] ->   3      3      correct \n",
      "           [ 0.  1. 12. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  3. 12.] ->   0      0      correct \n",
      "           [ 0.  0.  6. 16.] ->   0      0      correct \n",
      "           [ 0.  0. 15. 13.] ->   5      5      correct \n",
      "           [ 0.  0.  3. 11.] ->   0      0      correct \n",
      "           [ 0.  1. 10. 13.] ->   2      2      correct \n",
      "           [ 0.  0. 13. 15.] ->   5      5      correct \n",
      "               [0. 0. 0. 4.] ->   4      4      correct \n",
      "           [ 0.  1. 14. 14.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 15.] ->   8      8      correct \n",
      "               [0. 0. 9. 9.] ->   8      5      incorrect: [1.94284032e-05 2.82489009e-03 3.84966130e-03 2.90935437e-01\n",
      " 2.06460316e-06 3.02011520e-01 2.23624704e-04 1.63971049e-04\n",
      " 3.99926940e-01 4.24633230e-05]\n",
      "           [ 0.  0.  0. 11.] ->   6      6      correct \n",
      "           [ 0.  0.  4. 12.] ->   6      6      correct \n",
      "           [ 0.  0.  2. 13.] ->   6      6      correct \n",
      "               [0. 0. 0. 2.] ->   4      4      correct \n",
      "               [0. 0. 1. 5.] ->   3      9      incorrect: [1.01866408e-07 3.25231599e-03 1.99618711e-05 6.48865785e-01\n",
      " 1.95776516e-06 4.70557001e-04 8.42761177e-06 1.50934405e-03\n",
      " 4.55648973e-02 3.00306651e-01]\n",
      "           [ 0.  1. 12. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  1. 12.] ->   4      4      correct \n",
      "               [0. 0. 0. 9.] ->   6      6      correct \n",
      "               [0. 0. 0. 3.] ->   1      1      correct \n",
      "               [0. 0. 0. 0.] ->   1      1      correct \n",
      "           [ 0.  0.  5. 16.] ->   2      2      correct \n",
      "               [0. 0. 0. 2.] ->   1      8      incorrect: [2.97145844e-06 9.99292979e-01 1.42826015e-04 1.02818970e-05\n",
      " 1.41069217e-04 9.05869412e-06 4.42303013e-05 1.06529344e-09\n",
      " 3.48283546e-04 8.29909037e-06]\n",
      "               [0. 0. 0. 9.] ->   6      6      correct \n",
      "               [0. 0. 3. 9.] ->   5      5      correct \n",
      "           [ 0.  0.  1. 15.] ->   1      1      correct \n",
      "           [ 0.  3. 16. 13.] ->   5      5      correct \n",
      "           [ 0.  1. 11. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  8. 15.] ->   8      8      correct \n",
      "           [ 0.  0. 11. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  4. 10.] ->   7      7      correct \n",
      "               [0. 0. 0. 4.] ->   4      4      correct \n",
      "           [ 0.  0.  5. 12.] ->   7      8      incorrect: [4.05683839e-04 1.19924241e-05 3.80331163e-02 2.70533257e-01\n",
      " 3.50812148e-06 1.15731763e-02 3.92198465e-03 5.97457736e-01\n",
      " 7.80024277e-02 5.71172970e-05]\n",
      "           [ 0.  0.  4. 14.] ->   0      0      correct \n",
      "           [ 0.  0.  4. 12.] ->   8      2      incorrect: [1.26299358e-03 6.11379056e-05 6.20478350e-02 8.97444579e-02\n",
      " 1.04525605e-05 2.97780944e-02 8.68615690e-03 1.89284030e-01\n",
      " 6.19101481e-01 2.33608178e-05]\n",
      "               [0. 0. 1. 8.] ->   7      7      correct \n",
      "           [ 0.  0.  3. 15.] ->   6      6      correct \n",
      "           [ 0.  0.  6. 11.] ->   9      9      correct \n",
      "           [ 0.  0. 14. 16.] ->   8      8      correct \n",
      "           [ 0.  0.  2. 13.] ->   7      7      correct \n",
      "           [ 0.  0.  2. 15.] ->   9      7      incorrect: [1.11889899e-06 4.52554701e-04 5.97878338e-06 3.79023684e-04\n",
      " 5.76993868e-03 5.92330696e-04 6.60002656e-05 4.91671799e-01\n",
      " 2.00259742e-03 4.99058657e-01]\n",
      "           [ 0.  0.  4. 14.] ->   2      2      correct \n",
      "           [ 0.  0.  3. 15.] ->   4      4      correct \n",
      "           [ 0.  0.  2. 10.] ->   9      9      correct \n",
      "           [ 0.  0.  6. 15.] ->   7      7      correct \n",
      "               [0. 0. 0. 0.] ->   4      4      correct \n",
      "           [ 0.  0.  0. 16.] ->   6      6      correct \n",
      "               [0. 0. 0. 8.] ->   4      4      correct \n",
      "               [0. 0. 0. 7.] ->   8      8      correct \n",
      "           [ 0.  0.  9. 16.] ->   0      0      correct \n",
      "           [ 0.  0.  4. 12.] ->   0      0      correct \n",
      "               [0. 0. 0. 2.] ->   4      4      correct \n",
      "           [ 0.  0.  6. 16.] ->   1      1      correct \n",
      "           [ 0.  2.  9. 15.] ->   3      3      correct \n",
      "           [ 0.  0.  0. 13.] ->   6      6      correct \n",
      "           [ 0.  0.  2. 10.] ->   8      8      correct \n",
      "           [ 0.  0.  1. 10.] ->   6      6      correct \n",
      "           [ 0.  1. 13. 16.] ->   2      2      correct \n",
      "               [0. 0. 3. 6.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 13.] ->   6      6      correct \n",
      "           [ 0.  0.  4. 13.] ->   0      0      correct \n",
      "           [ 0.  0.  9. 15.] ->   7      7      correct \n",
      "               [0. 0. 0. 0.] ->   1      1      correct \n",
      "           [ 0.  0.  2. 15.] ->   7      7      correct \n",
      "               [0. 0. 2. 9.] ->   0      0      correct \n",
      "           [ 0.  0.  3. 12.] ->   8      8      correct \n",
      "           [ 0.  0. 10. 15.] ->   1      1      correct \n",
      "               [0. 0. 0. 3.] ->   9      9      correct \n",
      "           [ 0.  0.  9. 15.] ->   9      9      correct \n",
      "               [0. 1. 8. 8.] ->   5      5      correct \n",
      "           [ 0.  0.  5. 12.] ->   5      5      correct \n",
      "           [ 0.  0.  1. 11.] ->   6      6      correct \n",
      "           [ 0.  1. 14. 16.] ->   9      8      incorrect: [6.47940111e-03 9.33192864e-06 4.94400997e-03 8.44218135e-02\n",
      " 2.47786991e-06 1.55376232e-05 8.47493427e-06 1.15107732e-01\n",
      " 4.35150466e-02 7.45496174e-01]\n",
      "           [ 0.  1. 11. 16.] ->   1      1      correct \n",
      "           [ 0.  0. 11. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  7. 15.] ->   2      2      correct \n",
      "           [ 0.  0.  1. 14.] ->   1      1      correct \n",
      "               [0. 0. 9. 7.] ->   6      6      correct \n",
      "           [ 0.  0.  0. 10.] ->   1      8      incorrect: [1.26528846e-07 9.86735514e-01 5.41718807e-05 3.39056026e-05\n",
      " 2.60734153e-05 9.39917236e-07 1.14964550e-05 5.70752496e-09\n",
      " 1.30743532e-02 6.34136345e-05]\n",
      "           [ 0.  0.  3. 10.] ->   9      9      correct \n",
      "               [0. 0. 0. 1.] ->   1      1      correct \n",
      "           [ 0.  1. 12. 16.] ->   3      3      correct \n",
      "           [ 0.  0. 10. 16.] ->   3      3      correct \n",
      "           [ 0.  2. 15. 15.] ->   3      3      correct \n",
      "               [0. 0. 9. 8.] ->   5      5      correct \n",
      "           [ 0.  0.  6. 13.] ->   2      2      correct \n",
      "           [ 0.  0.  4. 12.] ->   1      1      correct \n",
      "           [ 0.  0.  4. 16.] ->   0      0      correct \n",
      "               [0. 0. 0. 1.] ->   6      6      correct \n",
      "           [ 0.  1. 12. 16.] ->   9      9      correct \n",
      "           [ 0.  0.  2. 15.] ->   1      1      correct \n",
      "           [ 0.  0.  3. 12.] ->   0      0      correct \n",
      "               [0. 0. 0. 3.] ->   4      4      correct \n",
      "           [ 0.  0.  5. 12.] ->   7      7      correct \n",
      "           [ 0.  0.  2. 10.] ->   0      0      correct \n",
      "           [ 0.  0.  9. 13.] ->   0      0      correct \n",
      "           [ 0.  0.  0. 10.] ->   6      6      correct \n",
      "           [ 0.  0.  6. 16.] ->   2      3      incorrect: [1.92539023e-06 1.33019047e-05 8.59715292e-01 1.39038469e-01\n",
      " 1.48897737e-07 1.95036722e-05 4.36189932e-09 6.94619267e-05\n",
      " 1.36188340e-04 1.00570446e-03]\n",
      "           [ 0.  0.  4. 14.] ->   1      1      correct \n",
      "           [ 0.  0.  0. 10.] ->   8      6      incorrect: [1.36093346e-08 8.54348193e-03 1.62771074e-08 3.17895402e-05\n",
      " 2.64606713e-07 7.11786685e-07 1.27041232e-03 9.52453362e-07\n",
      " 9.90144947e-01 7.41000481e-06]\n",
      "           [ 0.  0.  6. 10.] ->   8      8      correct \n",
      "           [ 0.  0.  8. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  4. 13.] ->   7      7      correct \n",
      "               [0. 0. 3. 8.] ->   5      5      correct \n",
      "               [0. 0. 0. 1.] ->   9      9      correct \n",
      "           [ 0.  0. 11. 10.] ->   5      5      correct \n",
      "           [ 0.  0.  7. 15.] ->   7      7      correct \n",
      "           [ 0.  0.  1. 12.] ->   9      9      correct \n",
      "               [0. 0. 0. 0.] ->   8      9      incorrect: [5.04184214e-09 4.75820971e-02 1.14800799e-07 4.04946550e-04\n",
      " 5.77020393e-07 1.04854191e-05 1.53704614e-04 7.18023742e-07\n",
      " 9.51781529e-01 6.58228547e-05]\n",
      "           [ 0.  0.  7. 16.] ->   5      5      correct \n",
      "           [ 0.  1. 10. 16.] ->   2      2      correct \n",
      "           [ 0.  0.  8. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  3. 12.] ->   6      6      correct \n",
      "           [ 0.  0.  4. 16.] ->   1      1      correct \n",
      "           [ 0.  0.  2. 10.] ->   8      8      correct \n",
      "           [ 0.  0.  2. 16.] ->   1      1      correct \n",
      "           [ 0.  0.  3. 16.] ->   4      4      correct \n",
      "           [ 0.  0.  9. 13.] ->   9      9      correct \n",
      "           [ 0.  1. 15. 16.] ->   5      5      correct \n",
      "           [ 0.  0. 10. 12.] ->   5      5      correct \n",
      "           [ 0.  1. 12. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  7. 14.] ->   3      9      incorrect: [3.50581043e-06 4.30374534e-05 6.25760172e-02 8.84883043e-01\n",
      " 5.83433600e-08 1.94606953e-05 5.03364166e-09 7.18715793e-05\n",
      " 1.38258306e-03 5.10204176e-02]\n",
      "               [0. 0. 0. 6.] ->   3      3      correct \n",
      "           [ 0.  0.  3. 12.] ->   8      8      correct \n",
      "           [ 0.  0.  5. 16.] ->   7      7      correct \n",
      "           [ 0.  0.  7. 13.] ->   2      2      correct \n",
      "           [ 0.  0.  2. 14.] ->   6      6      correct \n",
      "           [ 0.  0.  2. 12.] ->   1      1      correct \n",
      "           [ 0.  0.  4. 14.] ->   6      6      correct \n",
      "               [0. 0. 0. 5.] ->   4      4      correct \n",
      "           [ 0.  0.  3. 12.] ->   0      0      correct \n",
      "           [ 0.  2. 14. 16.] ->   5      5      correct \n",
      "           [ 0.  0. 10. 16.] ->   5      5      correct \n",
      "           [ 0.  0.  2. 16.] ->   6      6      correct \n",
      "           [ 0.  0.  3. 14.] ->   6      6      correct \n",
      "               [0. 0. 0. 8.] ->   7      7      correct \n",
      "           [ 0.  0.  0. 13.] ->   4      4      correct \n",
      "           [ 0.  0.  7. 15.] ->   3      3      correct \n",
      "           [ 0.  0.  6. 16.] ->   7      7      correct \n",
      "           [ 0.  1. 13. 16.] ->   3      3      correct \n",
      "           [ 0.  0.  9. 16.] ->   8      8      correct \n",
      "           [ 0.  0. 10. 16.] ->   3      3      correct \n",
      "           [ 0.  3. 12. 15.] ->   3      3      correct \n",
      "               [0. 0. 0. 0.] ->   8      9      incorrect: [9.53223516e-07 1.10411593e-02 1.69092969e-06 5.64684715e-03\n",
      " 1.78665658e-05 1.18731488e-03 5.88899140e-05 3.55536660e-04\n",
      " 9.39877891e-01 4.18118508e-02]\n",
      "           [ 0.  0.  1. 10.] ->   6      6      correct \n",
      "           [ 0.  2. 10. 14.] ->   3      3      correct \n",
      "           [ 0.  0.  8. 16.] ->   9      7      incorrect: [6.04819428e-04 7.35250505e-05 8.31636929e-03 2.78354667e-05\n",
      " 3.06441973e-02 3.65086792e-06 6.61315258e-06 4.21184898e-01\n",
      " 2.11512190e-04 5.38926579e-01]\n",
      "\n",
      "correct predictions: 320 out of 354\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 1.79261833e-01  1.39315663e-01  2.18116085e-01  2.20044883e-02\n",
      "  -2.93112067e-02 -1.93199542e-01]\n",
      " [ 1.32358863e-02 -4.23708504e-02 -1.15499784e-01 -3.60440168e-01\n",
      "  -2.76587945e-01 -2.27840248e-01]\n",
      " [-3.74228939e-01  4.39946392e-02 -1.24730134e-01  6.12202009e-01\n",
      "  -2.16904570e-01  2.44911582e-02]\n",
      " [-1.29617076e-02 -5.70143160e-01 -3.61389984e-01 -1.65203315e-01\n",
      "   4.30240691e-01  7.10563742e-01]\n",
      " [-5.18576570e-01 -2.29431998e-01  1.64340778e-01  7.49652933e-01\n",
      "   8.23640898e-02  1.68616803e-01]\n",
      " [-1.28853551e-01 -1.68253261e-01 -8.80761595e-01  1.31244289e+00\n",
      "  -4.68526915e-01 -7.79349667e-01]\n",
      " [-5.84713751e-01  4.56161642e-01 -2.15194657e-01 -2.20337804e-01\n",
      "  -2.58980892e-01  2.53042558e-01]\n",
      " [ 5.18413770e-02 -4.02012986e-01  5.80409161e-01 -1.03170942e-01\n",
      "   9.34200066e-01  3.22752382e-01]\n",
      " [ 6.38082563e-02  3.60651359e-01 -2.32195905e-01  4.98784406e-01\n",
      "   2.36050453e-01 -1.82432295e-01]\n",
      " [-3.71613156e-01  1.81913958e-01  3.63441173e-01 -4.92456808e-01\n",
      "   7.30812233e-01  4.31953347e-02]\n",
      " [-6.93831965e-01 -5.15211555e-01  1.42467521e-02  3.53834771e-01\n",
      "   5.08195628e-01  4.44783280e-01]\n",
      " [-5.80971651e-01  2.40407965e-01 -4.78206718e-01  9.00230790e-01\n",
      "   5.30177594e-02  4.22458038e-01]\n",
      " [-6.58539006e-01 -4.59564780e-01  9.14802010e-02 -4.42908826e-01\n",
      "   7.14820151e-01 -2.99712851e-01]\n",
      " [ 5.14440110e-01 -7.63423302e-01 -1.04871682e-01  2.58079580e-01\n",
      "   6.68013278e-03  1.47836473e-01]\n",
      " [ 4.28236199e-01 -7.42269356e-02 -2.89590960e-01  5.99112360e-01\n",
      "   1.28301322e-01  1.97116950e-01]\n",
      " [-3.88849815e-01 -2.14504008e-01  3.83837399e-01  6.22886995e-02\n",
      "   6.02162901e-02  5.07779484e-01]\n",
      " [ 2.27357609e-01 -6.79961715e-02  3.94258442e-02  6.15960816e-02\n",
      "   8.98752787e-03  6.51808672e-02]\n",
      " [ 1.55943314e-01  5.10976090e-01 -3.53944384e-01 -5.49426249e-03\n",
      "  -1.62272767e-01 -4.36348969e-02]\n",
      " [ 9.34027967e-01  5.03965527e-01 -8.46978467e-02 -6.41261109e-01\n",
      "  -4.03148227e-01  1.30174448e-01]\n",
      " [ 2.14248311e-01  4.97239982e-01 -2.40957007e-01 -6.66760446e-01\n",
      "  -1.20133229e+00 -1.76140470e-01]\n",
      " [ 1.12333414e-02 -1.69548616e+00 -4.39767045e-01 -9.73069520e-01\n",
      "  -2.90281892e-01  4.00767063e-01]\n",
      " [ 7.61679595e-01 -1.34825033e+00  4.44659257e-01 -1.37560317e+00\n",
      "   1.52612096e+00  9.56197106e-01]\n",
      " [ 2.53839416e-01 -1.00946771e+00  7.94511204e-01 -4.81386373e-01\n",
      "   1.56779681e-01 -1.64587638e-01]\n",
      " [-4.07368357e-01  2.42620578e-01 -3.00312263e-01 -7.12140950e-02\n",
      "  -1.79851343e-01 -6.70019408e-03]\n",
      " [ 1.70505261e-01  2.24641893e-01  2.46690392e-01 -3.35380801e-01\n",
      "  -3.08907781e-01  8.01036988e-02]\n",
      " [-2.57621593e-01  2.36736086e-01 -9.49673129e-02 -3.11537928e-01\n",
      "  -1.07072384e+00  1.61013150e-01]\n",
      " [ 4.02343991e-01  6.92562911e-02 -4.67997775e-02  1.12475677e-02\n",
      "  -1.02278016e+00  2.73017057e-01]\n",
      " [ 1.13797368e+00 -7.75355355e-01  2.94415940e-02  1.92952872e-01\n",
      "  -8.32483378e-01 -1.85059620e-01]\n",
      " [-5.26059161e-01 -4.32847672e-01 -5.78085144e-01 -5.17371847e-01\n",
      "  -5.88437812e-01  3.46550935e-02]\n",
      " [-1.65892262e-01 -2.93830434e-01 -1.56688668e-01 -7.07966303e-01\n",
      "   5.55283202e-02  4.09366100e-01]\n",
      " [-5.67330346e-01  1.79282544e-01  1.14259230e+00 -1.18495703e+00\n",
      "   7.92041075e-01  1.05890865e+00]\n",
      " [ 7.32321179e-02  4.60346318e-01  1.87318719e-01 -2.30695376e-01\n",
      "  -7.73368404e-01 -3.27126624e-01]\n",
      " [ 1.77684790e-01  8.33519744e-03  6.53746303e-02 -2.28254091e-01\n",
      "  -5.10569106e-02 -1.57855346e-01]\n",
      " [-6.43419423e-01  1.18585185e+00  1.04671125e+00 -6.39305186e-01\n",
      "   6.15722406e-01 -2.80151228e-02]\n",
      " [ 4.57463067e-01  9.04515077e-02  1.45829232e-01  3.55974147e-01\n",
      "  -5.81350822e-01 -2.28403847e-01]\n",
      " [ 1.75295334e-02 -1.02136289e+00  9.13431995e-01  5.31000708e-01\n",
      "   4.85887510e-01  1.72787625e-01]\n",
      " [-2.42487993e-03 -4.61497047e-01  7.97329146e-01  2.88490787e-01\n",
      "  -5.52669716e-01 -6.58755549e-01]\n",
      " [-2.72720024e-01  2.50716801e-01  4.59613064e-02  6.52652835e-01\n",
      "  -9.05685291e-01  4.11296827e-01]\n",
      " [-1.07208133e-02  4.50342039e-01  6.92158210e-01  2.10377566e-02\n",
      "   8.01647185e-01  2.43627045e-01]\n",
      " [-2.88498653e-01  6.58706822e-02  2.17079043e-01 -2.75554330e-01\n",
      "   2.74081154e-01 -1.54952849e-01]\n",
      " [ 8.84210643e-02  2.33527132e-01 -5.50088295e-02 -4.35283014e-02\n",
      "  -7.22467018e-02 -1.20848220e-01]\n",
      " [-4.48854328e-01  5.67954718e-01  3.33994851e-02 -6.96968427e-01\n",
      "  -3.60732701e-01  6.19594533e-02]\n",
      " [ 1.13910664e+00  9.25032678e-02  1.82919911e+00  1.97892774e-01\n",
      "   8.38918753e-01  8.75607588e-02]\n",
      " [-2.67121071e-01  4.19301435e-01  1.57304387e+00 -1.17228928e+00\n",
      "  -4.12776874e-01 -1.08410068e+00]\n",
      " [ 6.86391835e-01  5.71619373e-01  2.32245719e-01  7.03928806e-01\n",
      "  -1.00259380e-01 -6.99894623e-03]\n",
      " [ 7.57038930e-01  4.97300198e-01 -7.55426572e-02  9.69202762e-01\n",
      "  -3.07654413e-02  4.03769149e-01]\n",
      " [ 6.66968457e-01 -3.92812765e-01 -6.36389278e-01  2.69559292e-01\n",
      "  -3.23316585e-02 -3.06822025e-01]\n",
      " [-8.80168337e-02  3.62194629e-01  4.52831667e-01  1.91411753e-04\n",
      "  -9.53367398e-02  8.50591118e-02]\n",
      " [ 9.87286163e-02  6.22970246e-02  2.38883364e-01  3.61300165e-02\n",
      "  -2.04454767e-01  1.81025355e-01]\n",
      " [-1.62596640e-01  3.85489938e-01 -1.09872682e-01 -1.73350323e-01\n",
      "  -4.28376531e-01  3.43250938e-01]\n",
      " [ 5.97109859e-01 -6.04562065e-02  9.76798855e-01  1.04494289e-01\n",
      "   7.56883136e-01 -9.75506459e-01]\n",
      " [-5.98658905e-01 -3.75732223e-03  2.98607202e-01  2.56927700e-01\n",
      "   1.14052764e-01  1.59942462e-01]\n",
      " [-6.14443443e-02  8.41106415e-01 -1.20804597e+00 -1.53636459e+00\n",
      "  -1.29056336e-02 -7.54016991e-01]\n",
      " [ 1.28966045e-01  5.45013047e-01 -2.52665236e-01 -1.65973616e-01\n",
      "   5.91036615e-01 -1.04193560e+00]\n",
      " [-2.87704981e-01 -3.13082832e-01 -3.42612118e-02  4.60951704e-01\n",
      "  -3.05141319e-01  3.89601663e-03]\n",
      " [ 2.82421987e-01 -6.83361687e-02 -2.54709205e-01  3.34599252e-02\n",
      "   1.55305457e-01 -2.40192980e-01]\n",
      " [ 2.88697580e-01 -2.53454009e-01 -2.20008059e-01  2.18868625e-01\n",
      "   2.74208962e-01  2.81992003e-01]\n",
      " [ 1.29795045e-01  2.55217893e-02 -5.88755455e-01  3.34862588e-01\n",
      "   1.88808603e-01  2.82867999e-02]\n",
      " [-8.15346270e-01  4.83798411e-01 -2.41075570e-01  1.31743459e-01\n",
      "   2.99524425e-01  3.77594679e-01]\n",
      " [ 1.19770024e-01  3.30980320e-01 -1.71992435e-01  1.06475439e-01\n",
      "   1.27212898e-01 -1.86142394e-01]\n",
      " [ 1.31284986e+00 -1.80364847e-01 -1.08856382e+00 -1.04841785e+00\n",
      "  -4.57233829e-02 -4.11156026e-02]\n",
      " [ 2.93529453e-01  4.37605791e-02 -2.27776002e-01 -3.33693605e-01\n",
      "  -4.06797292e-01 -5.72807067e-01]\n",
      " [-4.53373202e-01  8.64806666e-01 -3.76069035e-01 -5.50010018e-01\n",
      "   2.00814412e-01  5.42448944e-02]\n",
      " [ 5.44695502e-01  3.05121823e-02 -3.02304083e-01 -2.62286234e-01\n",
      "   1.76403900e-01 -2.82135325e-01]]\n",
      "[[-0.43213474  1.33507729  0.93547547  2.48473919 -0.30855394 -0.45797537\n",
      "  -0.2530833 ]\n",
      " [-0.70372054  0.54245872  1.65441386 -1.91128792  2.61921919  0.88040641\n",
      "  -1.07035417]\n",
      " [ 0.45688142  2.00139287  1.35043869 -1.0879874   0.65539587 -1.06915205\n",
      "  -1.00959364]\n",
      " [ 2.1595369   0.69048399 -0.29887424  0.07275554 -1.57615238  1.0047623\n",
      "   0.91318837]\n",
      " [-1.49403607  1.60642753 -1.02687129 -0.14752531 -1.16155193  2.41549383\n",
      "   0.89007897]\n",
      " [-0.60940221  0.45639231 -1.45927353  1.11057463 -0.51718364 -0.65410551\n",
      "  -3.11529417]]\n",
      "[[-1.64033446 -0.79618619 -3.21511192  1.24365207 -1.40042227  2.45492375\n",
      "   1.82488948  1.15906498  0.59253914 -0.85300923]\n",
      " [ 2.27114602 -1.20446423 -1.21926049 -1.84834884  0.02343446 -1.96244744\n",
      "   3.19731248  1.45239217  0.98269915 -1.35009526]\n",
      " [ 1.07512694  1.26845221 -0.89476146 -2.19044351  0.60033053  0.68454394\n",
      "   1.43902403 -1.65310041  2.58402403 -2.06538079]\n",
      " [-0.52691229  1.33173907 -2.27370432  1.73204503 -2.13201377 -1.01745141\n",
      "  -0.47517692 -1.80217788  1.97087317  2.35621998]\n",
      " [ 1.55286282  1.23633581  0.01961718 -0.96807527  1.54932152  1.69851909\n",
      "   2.13535476 -2.25700437 -3.0071261  -2.18382019]\n",
      " [ 2.4052425  -3.45826081  2.07440804  2.42320646 -2.99231633  2.27521336\n",
      "  -2.04233096  0.80840744 -0.29928175 -0.5903568 ]\n",
      " [-1.74283434  2.43850907  2.14258782  2.73638349 -1.9740445  -0.02334291\n",
      "   1.11342962 -2.59115987  1.49728206 -2.39874699]]\n",
      "\n",
      "intercepts: [array([-0.73410979, -0.04750592,  0.63591406,  0.42958332,  0.15286114,\n",
      "       -0.97586041]), array([-0.79238487, -1.12604122,  1.32025412,  0.18135399,  0.18892296,\n",
      "        0.82308417, -0.39933985]), array([-0.48787508,  0.68293632,  0.20738244, -0.3716042 , -0.19752372,\n",
      "       -0.36383268,  0.22047762,  0.76746396,  1.49608555,  0.34962296])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We don't usually look inside the NNet, but we can: it's open-box modeling...\n",
    "#\n",
    "if True:  # do we want to see all of the parameters?\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [0, 0, 0, 5, 14, 12, 2, 0, 0, 0, 7, 15, 8, 14, 4, 0, 0, 0, 6, 2, 3, 13, 1, 0, 0, 0, 0, 1, 13, 4, 0, 0, 0, 0, 1, 11, 9, 0, 0, 0, 0, 8, 16, 13, 0, 0, 0, 0, 0, 5, 14, 16, 11, 2, 0, 0, 0, 0, 0, 6, 12, 13, 3, 0]\n",
      "nn.predict_proba ==  [[5.82118578e-04 1.71803095e-04 9.97973002e-01 1.60729918e-04\n",
      "  2.73751402e-05 3.52855362e-04 9.44946841e-06 1.02366120e-05\n",
      "  7.11888102e-04 5.41311793e-07]]\n",
      "prediction: [2.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [0,0,0,5,14,12,2,0,0,0,7,15,8,14,4,0,0,0,6,2,3,13,1,0,0,0,0,1,13,4,0,0,0,0,1,11,9,0,0,0,0,8,16,13,0,0,0,0,0,5,14,16,11,2,0,0,0,0,0,6,12,13,3,0]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")     # takes the max (nice to see them all!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now try to predict Pixel 42 while using the information from the other 63 pixels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'actual_digit': 64}\n",
      "Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63',\n",
      "       'actual_digit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# What shall we predict today?\n",
    "print(COL_INDEX)\n",
    "print(COLUMNS)\n",
    "\n",
    "# Let's first predict sepal length ('sepallen', column index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "y_all is \n",
      " [16.  0. 16. ... 16.  0. 16.]\n",
      "X_all (just features: first few rows) is \n",
      " [[ 0.  0.  9. 14.  8.  1.  0.  0.  0.  0. 12. 14. 14. 12.  0.  0.  0.  0.\n",
      "   9. 10.  0. 15.  4.  0.  0.  0.  3. 16. 12. 14.  2.  0.  0.  0.  4. 16.\n",
      "  16.  2.  0.  0.  0.  3.  8. 10. 13.  2.  0.  0.  1. 15.  1.  3. 16.  8.\n",
      "   0.  0.  0. 11. 16. 15. 11.  1.  0.  8.]\n",
      " [ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      "  16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "   9. 15.  2.  0.  0.  0.  3.  0.  9. 11.  0.  0.  0.  0.  0.  9. 15.  4.\n",
      "   0.  0.  0.  9. 12. 13.  3.  0.  0.  9.]\n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      "  16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "   0.  8.  8.  0.  0.  1.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.  0.\n",
      "   0.  0.  0.  1. 10. 13.  3.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 14. 13.  1.  0.  0.  0.  0.  5. 16. 16.  2.  0.  0.  0.\n",
      "   0. 14. 16. 12.  0.  0.  0.  1. 10. 16. 16. 12.  0.  0.  0.  3. 12. 14.\n",
      "  16.  9.  0.  0.  0.  0.  5. 16. 15.  0.  0.  0.  0.  0.  4. 16. 14.  0.\n",
      "   0.  0.  0.  0.  1. 13. 16.  1.  0.  1.]\n",
      " [ 0.  0.  5. 12.  1.  0.  0.  0.  0.  0. 15. 14.  7.  0.  0.  0.  0.  0.\n",
      "  13.  1. 12.  0.  0.  0.  0.  2. 10.  0. 14.  0.  0.  0.  0.  0.  2.  0.\n",
      "  16.  1.  0.  0.  0.  0.  6. 15.  0.  0.  0.  0.  0.  9. 16. 15.  9.  8.\n",
      "   2.  0.  0.  3. 11.  8. 13. 12.  4.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'sepallen'  (column index 0)  using\n",
    "#\n",
    "\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "X_all = np.concatenate( (A[:,0:42], A[:,43:]),axis=1)  # columns 1, 2, 3, and 4\n",
    "\n",
    "# if we wanted all-except-column 1:   X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "# if we wanted all-except-column 2:   X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "# if we wanted all-except-column 3:   X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4\n",
    "# if we wanted all-except-column 4:   X_all = np.concatenate( (A[:,0:4], A[:,5:]),axis=1)  # columns 0, 1, 2, and 3\n",
    "# slicing is forgiving...\n",
    "\n",
    "\n",
    "y_all = A[:,42]             # y (labels) ... is all of column 0, sepallen (sepal length)  Re-index, as needed...\n",
    "print(f\"y_all is \\n {y_all}\") \n",
    "print(f\"X_all (just features: first few rows) is \\n {X_all[:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [ 1.  0.  0. ... 16. 13. 10.]\n",
      "\n",
      "features (a few)\n",
      " [[ 0.  1. 12. 16. 16. 15.  0.  0.  0.  7. 13.  7.  8. 16.  0.  0.  0.  0.\n",
      "   1.  0.  8. 14.  0.  0.  0.  0.  7. 15. 16. 16. 11.  0.  0.  3. 15. 12.\n",
      "  15.  4.  2.  0.  0.  0. 12.  7.  0.  0.  0.  0.  0.  2. 16.  2.  0.  0.\n",
      "   0.  0.  0. 13.  9.  0.  0.  0.  0.  7.]\n",
      " [ 0.  0.  0.  1. 12. 16. 14.  0.  0.  0.  3. 14. 13. 15. 13.  0.  0.  4.\n",
      "  16. 15. 13. 16.  4.  0.  0.  3. 16. 16. 16. 16.  3.  0.  0.  0.  7.  7.\n",
      "  14. 14.  0.  0.  0.  0.  0. 12. 11.  0.  0.  0.  0.  0.  0. 13. 10.  0.\n",
      "   0.  0.  0.  0.  0. 13. 12.  0.  0.  9.]\n",
      " [ 0.  0.  9. 13.  6.  0.  0.  0.  0.  0. 14.  7. 11.  3.  0.  0.  0.  4.\n",
      "   7.  8.  5.  8.  0.  0.  0.  8. 10. 15. 14.  9.  0.  0.  0.  0.  4.  7.\n",
      "   9. 13.  1.  0.  0.  0.  0.  0.  5. 11.  0.  0.  0.  2.  0.  2. 12.  6.\n",
      "   0.  0.  0. 10. 14. 14.  7.  0.  0.  9.]\n",
      " [ 0.  3. 16. 15.  6.  0.  0.  0.  0.  5. 14. 14. 16.  0.  0.  0.  0.  0.\n",
      "   0.  6. 14.  0.  0.  0.  0.  0.  0. 13. 11.  0.  0.  0.  0.  0.  5. 16.\n",
      "   3.  0.  0.  0.  0.  1. 10.  0.  0.  0.  0.  0.  9. 16.  8.  8. 10.  5.\n",
      "   0.  0.  4. 16. 16. 16. 14.  3.  0.  2.]\n",
      " [ 0.  1. 10. 15. 15.  5.  0.  0.  0. 11. 16.  9. 12. 10.  0.  0.  0. 15.\n",
      "   6.  0. 14.  7.  0.  0.  0.  0.  0.  6. 16.  5.  0.  0.  0.  0.  1. 15.\n",
      "  11.  0.  0.  0.  0.  0. 16.  4.  0.  0.  0.  0.  1. 15. 11.  8. 12. 14.\n",
      "   1.  0.  1. 15. 16. 16. 12.  5.  0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 1414 rows;  testing with 354 rows\n",
      "\n",
      "Held-out data... (testing data: 354)\n",
      "y_test: [10. 16.  4.  3. 15.  0. 16.  0. 16.  1.  8.  0.  0. 10.  0.  0. 13. 14.\n",
      "  0.  0.  0.  4.  1.  0.  0.  0. 15.  0.  0. 16.  0. 13. 12. 12.  2. 16.\n",
      " 15.  0.  0.  0.  0. 12. 16.  0. 12.  0.  0.  0. 15.  0.  0.  0. 15.  5.\n",
      "  3.  0. 16. 16.  0.  0.  1.  0. 16. 15.  0.  0. 16. 16. 12.  0. 12.  0.\n",
      " 12.  8.  0.  0.  0.  0. 15. 16. 10.  0.  0. 15.  0. 16.  2.  0.  0.  9.\n",
      "  0.  0.  0.  0. 13.  0.  7.  0.  6.  0.  0. 14.  0.  9.  0. 16.  0.  2.\n",
      "  0. 16.  8. 11.  2.  0.  0. 16. 12. 16.  0. 10.  2.  3. 10. 11.  0.  0.\n",
      "  1. 11. 12. 14. 16.  0.  0. 16.  8. 16. 13.  0.  0.  0.  0.  0.  0. 12.\n",
      " 15.  1.  4.  0. 16. 15.  0. 14. 16.  9. 16.  7. 16.  0. 13.  0.  1. 13.\n",
      " 12.  3. 15.  0.  8.  0.  8. 15.  3. 12. 13. 13. 16.  0.  0. 15. 16.  3.\n",
      " 12. 14. 16.  0.  0.  0. 13.  0.  0. 12.  4.  3. 16.  8.  0.  7. 15. 12.\n",
      "  0. 15. 16.  0.  0.  0.  6.  6. 16.  1.  0.  0. 16.  1.  1.  0.  3. 12.\n",
      "  9. 13.  0. 15.  0.  0. 15.  0.  2.  1.  3.  6.  4.  0. 13.  1.  0.  0.\n",
      "  5.  0. 12.  1.  0. 16. 14. 11.  2.  7. 11.  5.  0.  4.  0.  0.  8. 14.\n",
      " 10.  0.  1.  6.  0. 13. 13.  9.  4.  0. 16.  0. 10. 16.  9.  4.  7. 10.\n",
      "  0.  0. 13. 16. 16. 16. 16. 16. 10. 15.  4.  0.  0.  1.  0.  0.  0. 13.\n",
      "  8. 11. 16. 16. 16.  0.  3.  0.  0. 16.  0.  0.  0. 15. 16.  0.  4. 15.\n",
      " 16. 14. 10. 11.  6.  3. 15. 10. 14. 16. 15. 16. 15. 13.  0. 13.  0. 16.\n",
      " 15. 11.  0. 13.  0.  7. 16. 12.  3. 13.  0. 11.  0. 15.  0.  0. 16.  0.\n",
      "  0.  0.  7. 13. 13. 15. 12. 14. 16.  2.  8.  0.]\n",
      "\n",
      "X_test (few rows): [[ 0.  1.  8. 16. 16.  3.  0.  0.  0.  6. 16. 12. 16.  4.  0.  0.  0.  1.\n",
      "   7.  0. 16.  4.  0.  0.  0.  0.  0.  7. 15.  0.  0.  0.  0.  0.  0. 14.\n",
      "   9.  0.  0.  0.  0.  0. 14.  1.  4.  5.  0.  0.  0. 13. 12. 11. 15.  3.\n",
      "   0.  0.  0. 12. 16. 12.  3.  0.  0.  2.]\n",
      " [ 0.  0.  4. 15. 14.  4.  0.  0.  0.  1. 14.  8. 10. 13.  1.  0.  0.  5.\n",
      "  13.  0.  0. 16.  3.  0.  0.  6. 12.  0.  0. 13.  3.  0.  0.  7. 12.  0.\n",
      "   0. 14.  3.  0.  0.  1.  0.  0. 14.  3.  0.  0.  0. 10. 11. 12. 14.  0.\n",
      "   0.  0.  0.  1. 11. 12.  3.  0.  0.  0.]\n",
      " [ 0.  0.  3. 16. 12. 12.  7.  0.  0.  0. 12. 13. 13. 16.  6.  0.  0.  0.\n",
      "   2.  0.  6. 14.  0.  0.  0.  0.  1.  4. 13. 10.  1.  0.  0.  0.  9. 16.\n",
      "  16. 16.  8.  0.  0.  0. 12. 12.  7.  1.  0.  0.  0.  0. 14.  6.  0.  0.\n",
      "   0.  0.  0.  4. 16.  2.  0.  0.  0.  7.]\n",
      " [ 0.  0.  5. 16. 12.  2.  0.  0.  0.  0.  4. 11. 16. 10.  0.  0.  0.  0.\n",
      "   0.  0. 14. 11.  0.  0.  0.  0.  2.  4. 14. 14.  2.  0.  0.  0. 13. 16.\n",
      "  16. 10.  4.  0.  0.  0. 10. 14.  0.  0.  0.  0.  0.  0. 15.  5.  0.  0.\n",
      "   0.  0.  0.  6. 11.  0.  0.  0.  0.  7.]\n",
      " [ 0.  0.  0.  8. 15. 10.  0.  0.  0.  0.  8. 13.  6.  1.  0.  0.  0.  1.\n",
      "  16.  2.  0.  0.  0.  0.  0.  4. 11.  0.  0.  0.  0.  0.  0.  4. 16. 12.\n",
      "  12.  9.  2.  0.  0.  1.  1.  0.  9. 10.  0.  0.  0. 10.  9.  4. 13.  3.\n",
      "   0.  0.  0.  0. 11. 15.  5.  0.  0.  6.]]\n",
      "\n",
      "Data used for modeling... (training data: 1414)\n",
      "y_train: [ 5.  2. 16. ...  0.  0. 16.]\n",
      "\n",
      "X_train (few rows): [[ 0.  1. 15. 16.  4.  0.  0.  0.  0.  9. 16. 11. 14.  0.  0.  0.  0. 12.\n",
      "  10.  5. 16.  0.  0.  0.  0.  4.  7.  8. 13.  0.  0.  0.  0.  0.  1. 15.\n",
      "   6.  0.  0.  0.  0.  0. 16.  2.  0.  0.  0.  0.  4. 15. 14. 10. 11. 12.\n",
      "   1.  0.  0. 13. 16. 16. 15. 11.  1.  2.]\n",
      " [ 0.  1.  9. 12. 16. 16.  4.  0.  0.  1. 11.  8.  7. 16.  4.  0.  0.  0.\n",
      "   0.  0.  8. 13.  0.  0.  0.  0.  5. 11. 15. 15.  9.  0.  0.  0. 16. 15.\n",
      "  13.  5.  2.  0.  0.  0. 16.  5.  0.  0.  0.  0.  0.  9. 14.  1.  0.  0.\n",
      "   0.  0.  0. 14. 10.  0.  0.  0.  0.  7.]\n",
      " [ 0.  0.  0.  0. 13.  8.  0.  0.  0.  0.  0.  5. 16.  3.  0.  0.  0.  0.\n",
      "   0. 14. 10.  2.  9.  0.  0.  1. 11. 13.  0. 10. 15.  0.  0. 12. 15.  5.\n",
      "   7. 14. 10.  0.  1. 15. 16. 16. 16.  4.  0.  0.  4.  4.  3. 10. 14.  0.\n",
      "   0.  0.  0.  0.  0. 15.  7.  0.  0.  4.]\n",
      " [ 0.  0.  0.  8. 15.  0.  0.  0.  0.  0.  5. 16.  6.  0.  0.  0.  0.  1.\n",
      "  14. 10.  0.  7.  7.  0.  0.  5. 16.  3.  1. 16.  7.  0.  0.  8. 16. 11.\n",
      "  13. 16.  3.  0.  0.  1. 15. 16. 13.  3.  0.  0.  0.  0.  7. 16.  1.  0.\n",
      "   0.  0.  0.  0. 12. 10.  0.  0.  0.  4.]\n",
      " [ 0.  0.  5. 16. 16. 10.  0.  0.  0.  2. 16. 14. 14. 14.  0.  0.  0.  2.\n",
      "  14.  4. 14. 10.  0.  0.  0.  0.  0.  8. 16.  8.  0.  0.  0.  0.  0.  0.\n",
      "   8. 16.  6.  0.  0.  0.  0.  0. 12. 13.  0.  0.  0.  7. 11.  8. 16. 11.\n",
      "   0.  0.  0.  8. 16. 16. 10.  1.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "             [0.         0.81075744 2.1068105  0.97479143] -> ?     5.00 \n",
      "             [0.         0.81075744 0.82752477 0.01793264] -> ?     2.00 \n",
      "         [ 0.         -0.31871155 -1.09140384 -2.85264373] -> ?     16.00\n",
      "         [ 0.         -0.31871155 -1.09140384 -0.93892615] -> ?     11.00\n",
      "         [ 0.         -0.31871155 -0.02533239  0.97479143] -> ?     0.00 \n",
      "\n",
      "                                                    input  -> pred  des. \n",
      "                                         [ 0.  1. 15. 16.] -> ?     5.00 \n",
      "                                         [ 0.  1.  9. 12.] -> ?     2.00 \n",
      "                                             [0. 0. 0. 0.] -> ?     16.00\n",
      "                                             [0. 0. 0. 8.] -> ?     11.00\n",
      "                                         [ 0.  0.  5. 16.] -> ?     0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 17.39624335\n",
      "Iteration 2, loss = 5.90293446\n",
      "Iteration 3, loss = 4.81063170\n",
      "Iteration 4, loss = 3.98438957\n",
      "Iteration 5, loss = 3.73605738\n",
      "Iteration 6, loss = 3.28778560\n",
      "Iteration 7, loss = 3.82157908\n",
      "Iteration 8, loss = 3.35736090\n",
      "Iteration 9, loss = 3.45558638\n",
      "Iteration 10, loss = 3.31614762\n",
      "Iteration 11, loss = 3.35625595\n",
      "Iteration 12, loss = 3.14600988\n",
      "Iteration 13, loss = 3.16221570\n",
      "Iteration 14, loss = 2.80380848\n",
      "Iteration 15, loss = 2.84037787\n",
      "Iteration 16, loss = 2.74785762\n",
      "Iteration 17, loss = 2.97325606\n",
      "Iteration 18, loss = 3.53305475\n",
      "Iteration 19, loss = 3.05678400\n",
      "Iteration 20, loss = 3.14486626\n",
      "Iteration 21, loss = 3.32952330\n",
      "Iteration 22, loss = 3.43786100\n",
      "Iteration 23, loss = 3.23211180\n",
      "Iteration 24, loss = 3.37614406\n",
      "Iteration 25, loss = 3.15917584\n",
      "Iteration 26, loss = 2.88195676\n",
      "Iteration 27, loss = 2.86104046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 28, loss = 3.09472202\n",
      "Iteration 29, loss = 3.11186779\n",
      "Iteration 30, loss = 2.79468123\n",
      "Iteration 31, loss = 2.69840212\n",
      "Iteration 32, loss = 2.61203649\n",
      "Iteration 33, loss = 2.53121522\n",
      "Iteration 34, loss = 2.48069447\n",
      "Iteration 35, loss = 2.45994645\n",
      "Iteration 36, loss = 2.46854370\n",
      "Iteration 37, loss = 2.46088041\n",
      "Iteration 38, loss = 2.41164014\n",
      "Iteration 39, loss = 2.37265026\n",
      "Iteration 40, loss = 2.34647502\n",
      "Iteration 41, loss = 2.35594671\n",
      "Iteration 42, loss = 2.31538944\n",
      "Iteration 43, loss = 2.29453986\n",
      "Iteration 44, loss = 2.30486812\n",
      "Iteration 45, loss = 2.29328328\n",
      "Iteration 46, loss = 2.26332744\n",
      "Iteration 47, loss = 2.27190143\n",
      "Iteration 48, loss = 2.25118142\n",
      "Iteration 49, loss = 2.24835967\n",
      "Iteration 50, loss = 2.22772442\n",
      "Iteration 51, loss = 2.21243873\n",
      "Iteration 52, loss = 2.20407921\n",
      "Iteration 53, loss = 2.19534395\n",
      "Iteration 54, loss = 2.18409548\n",
      "Iteration 55, loss = 2.25605880\n",
      "Iteration 56, loss = 2.27221319\n",
      "Iteration 57, loss = 2.22243070\n",
      "Iteration 58, loss = 2.25233423\n",
      "Iteration 59, loss = 2.18453365\n",
      "Iteration 60, loss = 2.14792941\n",
      "Iteration 61, loss = 2.11341717\n",
      "Iteration 62, loss = 2.06594557\n",
      "Iteration 63, loss = 2.05799342\n",
      "Iteration 64, loss = 2.03411382\n",
      "Iteration 65, loss = 2.03171816\n",
      "Iteration 66, loss = 2.02727017\n",
      "Iteration 67, loss = 2.00429493\n",
      "Iteration 68, loss = 2.00383047\n",
      "Iteration 69, loss = 1.98933601\n",
      "Iteration 70, loss = 1.97790865\n",
      "Iteration 71, loss = 1.97782825\n",
      "Iteration 72, loss = 1.97343541\n",
      "Iteration 73, loss = 1.95185835\n",
      "Iteration 74, loss = 1.93625752\n",
      "Iteration 75, loss = 1.96693674\n",
      "Iteration 76, loss = 1.96646653\n",
      "Iteration 77, loss = 1.94399868\n",
      "Iteration 78, loss = 1.95486799\n",
      "Iteration 79, loss = 1.92673358\n",
      "Iteration 80, loss = 1.91621593\n",
      "Iteration 81, loss = 1.90074861\n",
      "Iteration 82, loss = 1.87547937\n",
      "Iteration 83, loss = 1.86501917\n",
      "Iteration 84, loss = 1.85212575\n",
      "Iteration 85, loss = 1.85056431\n",
      "Iteration 86, loss = 1.85090061\n",
      "Iteration 87, loss = 1.84261723\n",
      "Iteration 88, loss = 1.81444010\n",
      "Iteration 89, loss = 1.81729146\n",
      "Iteration 90, loss = 1.79138964\n",
      "Iteration 91, loss = 1.79862776\n",
      "Iteration 92, loss = 1.79059279\n",
      "Iteration 93, loss = 1.80826529\n",
      "Iteration 94, loss = 1.78383324\n",
      "Iteration 95, loss = 1.80537927\n",
      "Iteration 96, loss = 1.85371699\n",
      "Iteration 97, loss = 1.84520899\n",
      "Iteration 98, loss = 1.82809060\n",
      "Iteration 99, loss = 1.79903405\n",
      "Iteration 100, loss = 1.80140851\n",
      "Iteration 101, loss = 1.78816285\n",
      "Iteration 102, loss = 1.77293911\n",
      "Iteration 103, loss = 1.77942544\n",
      "Iteration 104, loss = 1.77374240\n",
      "Iteration 105, loss = 1.77473827\n",
      "Iteration 106, loss = 1.76519084\n",
      "Iteration 107, loss = 1.75557822\n",
      "Iteration 108, loss = 1.76647599\n",
      "Iteration 109, loss = 1.74367580\n",
      "Iteration 110, loss = 1.77774228\n",
      "Iteration 111, loss = 1.77410868\n",
      "Iteration 112, loss = 1.76250445\n",
      "Iteration 113, loss = 1.75558946\n",
      "Iteration 114, loss = 1.74726035\n",
      "Iteration 115, loss = 1.73389902\n",
      "Iteration 116, loss = 1.72976608\n",
      "Iteration 117, loss = 1.72363868\n",
      "Iteration 118, loss = 1.71909207\n",
      "Iteration 119, loss = 1.72764669\n",
      "Iteration 120, loss = 1.72180724\n",
      "Iteration 121, loss = 1.72175479\n",
      "Iteration 122, loss = 1.72550152\n",
      "Iteration 123, loss = 1.79317980\n",
      "Iteration 124, loss = 1.83662202\n",
      "Iteration 125, loss = 1.82626898\n",
      "Iteration 126, loss = 1.78588534\n",
      "Iteration 127, loss = 1.76015468\n",
      "Iteration 128, loss = 1.75796207\n",
      "Iteration 129, loss = 1.75222708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 130, loss = 1.78209179\n",
      "Iteration 131, loss = 1.80568775\n",
      "Iteration 132, loss = 1.79290975\n",
      "Iteration 133, loss = 1.76964864\n",
      "Iteration 134, loss = 1.75593266\n",
      "Iteration 135, loss = 1.75471150\n",
      "Iteration 136, loss = 1.75173811\n",
      "Iteration 137, loss = 1.74162854\n",
      "Iteration 138, loss = 1.73144066\n",
      "Iteration 139, loss = 1.72601366\n",
      "Iteration 140, loss = 1.72224739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 141, loss = 1.71833716\n",
      "Iteration 142, loss = 1.71633861\n",
      "Iteration 143, loss = 1.71535489\n",
      "Iteration 144, loss = 1.71416594\n",
      "Iteration 145, loss = 1.71352871\n",
      "Iteration 146, loss = 1.71259747\n",
      "Iteration 147, loss = 1.71202957\n",
      "Iteration 148, loss = 1.71177630\n",
      "Iteration 149, loss = 1.71077581\n",
      "Iteration 150, loss = 1.71052194\n",
      "Iteration 151, loss = 1.71063747\n",
      "Iteration 152, loss = 1.71040877\n",
      "Iteration 153, loss = 1.70990121\n",
      "Iteration 154, loss = 1.70883391\n",
      "Iteration 155, loss = 1.70812138\n",
      "Iteration 156, loss = 1.70782950\n",
      "Iteration 157, loss = 1.70726188\n",
      "Iteration 158, loss = 1.70670256\n",
      "Iteration 159, loss = 1.70629238\n",
      "Iteration 160, loss = 1.70604216\n",
      "Iteration 161, loss = 1.70538036\n",
      "Iteration 162, loss = 1.70475568\n",
      "Iteration 163, loss = 1.70430466\n",
      "Iteration 164, loss = 1.70368301\n",
      "Iteration 165, loss = 1.70321765\n",
      "Iteration 166, loss = 1.70314679\n",
      "Iteration 167, loss = 1.70291482\n",
      "Iteration 168, loss = 1.70141214\n",
      "Iteration 169, loss = 1.69970473\n",
      "Iteration 170, loss = 1.69794945\n",
      "Iteration 171, loss = 1.69694356\n",
      "Iteration 172, loss = 1.69633930\n",
      "Iteration 173, loss = 1.69622039\n",
      "Iteration 174, loss = 1.69611883\n",
      "Iteration 175, loss = 1.69554394\n",
      "Iteration 176, loss = 1.69506165\n",
      "Iteration 177, loss = 1.69455592\n",
      "Iteration 178, loss = 1.69410269\n",
      "Iteration 179, loss = 1.69384028\n",
      "Iteration 180, loss = 1.69344864\n",
      "Iteration 181, loss = 1.69315398\n",
      "Iteration 182, loss = 1.69281270\n",
      "Iteration 183, loss = 1.69255303\n",
      "Iteration 184, loss = 1.69145829\n",
      "Iteration 185, loss = 1.69099226\n",
      "Iteration 186, loss = 1.69099438\n",
      "Iteration 187, loss = 1.69056296\n",
      "Iteration 188, loss = 1.69014298\n",
      "Iteration 189, loss = 1.68964325\n",
      "Iteration 190, loss = 1.68960022\n",
      "Iteration 191, loss = 1.68916429\n",
      "Iteration 192, loss = 1.68879974\n",
      "Iteration 193, loss = 1.68848195\n",
      "Iteration 194, loss = 1.68847495\n",
      "Iteration 195, loss = 1.68768849\n",
      "Iteration 196, loss = 1.68765160\n",
      "Iteration 197, loss = 1.68711328\n",
      "Iteration 198, loss = 1.68677008\n",
      "Iteration 199, loss = 1.68687440\n",
      "Iteration 200, loss = 1.68610204\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 1.6861020434240115\n",
      "And, its square root: 1.2984999204559127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "           [ 0.  1.  8. 16.] ->  +6.727  +10.000    3.273   \n",
      "           [ 0.  0.  4. 15.] ->  +14.123  +16.000    1.877   \n",
      "           [ 0.  0.  3. 16.] ->  +5.048  +4.000    1.048   \n",
      "           [ 0.  0.  5. 16.] ->  +0.100  +3.000    2.900   \n",
      "               [0. 0. 0. 8.] ->  +14.119  +15.000    0.881   \n",
      "               [0. 0. 0. 0.] ->  +1.852  +0.000    1.852   \n",
      "           [ 0.  0.  0. 10.] ->  +15.451  +16.000    0.549   \n",
      "           [ 0.  5. 16. 16.] ->  +0.111  +0.000    0.111   \n",
      "           [ 0.  0.  5. 11.] ->  +14.116  +16.000    1.884   \n",
      "           [ 0.  1. 15. 16.] ->  +3.049  +1.000    2.049   \n",
      "           [ 0.  0.  1. 10.] ->  +1.944  +8.000    6.056   \n",
      "           [ 0.  0.  9. 12.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0. 10. 16.] ->  +0.022  +0.000    0.022   \n",
      "           [ 0.  1. 10. 16.] ->  +8.102  +10.000    1.898   \n",
      "           [ 0.  0.  9. 16.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  4. 10.] ->  +0.082  +0.000    0.082   \n",
      "           [ 0.  0.  1. 10.] ->  +14.685  +13.000    1.685   \n",
      "           [ 0.  0.  4. 15.] ->  +15.452  +14.000    1.452   \n",
      "           [ 0.  0. 15. 13.] ->  +0.103  +0.000    0.103   \n",
      "           [ 0.  0.  9. 13.] ->  +2.036  +0.000    2.036   \n",
      "           [ 0.  0. 10. 16.] ->  +0.096  +0.000    0.096   \n",
      "               [0. 0. 0. 8.] ->  +0.854  +4.000    3.146   \n",
      "           [ 0.  0.  8. 14.] ->  +1.806  +1.000    0.806   \n",
      "               [0. 0. 0. 3.] ->  +0.355  +0.000    0.355   \n",
      "               [0. 0. 0. 1.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  2. 11. 14.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 10. 11.] ->  +10.890  +15.000    4.110   \n",
      "               [0. 0. 0. 9.] ->  +0.166  +0.000    0.166   \n",
      "           [ 0.  0.  9. 16.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  0. 12.] ->  +14.722  +16.000    1.278   \n",
      "           [ 0.  0.  5. 12.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  3. 13.] ->  +11.047  +13.000    1.953   \n",
      "               [0. 0. 0. 9.] ->  +10.114  +12.000    1.886   \n",
      "               [0. 0. 0. 5.] ->  +8.137  +12.000    3.863   \n",
      "           [ 0.  0. 10. 16.] ->  +0.229  +2.000    1.771   \n",
      "           [ 0.  0.  4. 15.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  0.  0. 10.] ->  +14.636  +15.000    0.364   \n",
      "           [ 0.  0. 10. 15.] ->  -0.042  +0.000    0.042   \n",
      "               [0. 0. 0. 8.] ->  +0.495  +0.000    0.495   \n",
      "           [ 0.  0.  4. 12.] ->  -0.039  +0.000    0.039   \n",
      "           [ 0.  1.  8. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  3. 15.] ->  +10.302  +12.000    1.698   \n",
      "           [ 0.  0.  0. 12.] ->  +15.659  +16.000    0.341   \n",
      "               [0. 0. 0. 2.] ->  +0.768  +0.000    0.768   \n",
      "               [0. 0. 0. 7.] ->  +5.210  +12.000    6.790   \n",
      "           [ 0.  0.  8. 13.] ->  +0.095  +0.000    0.095   \n",
      "               [0. 0. 0. 4.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0. 10. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  1. 12.] ->  +7.320  +15.000    7.680   \n",
      "           [ 0.  2. 13. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 13. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  7. 16.] ->  -0.042  +0.000    0.042   \n",
      "               [0. 0. 0. 8.] ->  +14.964  +15.000    0.036   \n",
      "           [ 0.  0.  2. 10.] ->  +13.641  +5.000    8.641   \n",
      "           [ 0.  0.  5. 13.] ->  +1.942  +3.000    1.058   \n",
      "           [ 0.  0.  7. 12.] ->  +2.558  +0.000    2.558   \n",
      "               [0. 0. 0. 7.] ->  +15.452  +16.000    0.548   \n",
      "               [0. 0. 0. 1.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  1. 10.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  2. 13. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 10. 16.] ->  +4.426  +1.000    3.426   \n",
      "           [ 0.  2.  9. 11.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  4. 14.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0. 11.  8.] ->  +13.569  +15.000    1.431   \n",
      "           [ 0.  0.  8. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  1.  9. 12.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  0. 14.] ->  +15.419  +16.000    0.581   \n",
      "           [ 0.  0.  6. 15.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  1. 13.] ->  +11.412  +12.000    0.588   \n",
      "           [ 0.  0.  3. 13.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  1. 13.  9.] ->  +0.703  +12.000    11.297  \n",
      "               [0. 0. 0. 0.] ->  +0.882  +0.000    0.882   \n",
      "               [0. 0. 0. 5.] ->  +15.452  +12.000    3.452   \n",
      "           [ 0.  0.  4. 15.] ->  +3.529  +8.000    4.471   \n",
      "           [ 0.  0. 11. 12.] ->  +0.378  +0.000    0.378   \n",
      "               [0. 0. 3. 9.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0. 13. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  6. 15.] ->  +3.928  +0.000    3.928   \n",
      "               [0. 0. 0. 7.] ->  +15.452  +15.000    0.452   \n",
      "           [ 0.  0.  4. 15.] ->  +12.057  +16.000    3.943   \n",
      "           [ 0.  0.  6. 16.] ->  +12.144  +10.000    2.144   \n",
      "           [ 0.  0.  7. 13.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  6. 12.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  5. 16.] ->  +14.115  +15.000    0.885   \n",
      "           [ 0.  0. 11. 16.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  5. 16.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  0.  4. 12.] ->  +6.446  +2.000    4.446   \n",
      "           [ 0.  1. 12. 16.] ->  +0.096  +0.000    0.096   \n",
      "               [0. 0. 0. 2.] ->  +0.377  +0.000    0.377   \n",
      "               [0. 0. 0. 0.] ->  +1.018  +9.000    7.982   \n",
      "           [ 0.  0. 12. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  2. 14. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 11. 16.] ->  +0.384  +0.000    0.384   \n",
      "           [ 0.  1. 11. 12.] ->  +3.440  +0.000    3.440   \n",
      "               [0. 0. 0. 6.] ->  +10.537  +13.000    2.463   \n",
      "           [ 0.  1. 10. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  2. 14. 16.] ->  +5.051  +7.000    1.949   \n",
      "           [ 0.  0.  7. 10.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  2. 15.] ->  +2.069  +6.000    3.931   \n",
      "           [ 0.  0.  2. 13.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  3. 16. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  2. 13.] ->  +7.400  +14.000    6.600   \n",
      "           [ 0.  1. 12. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  4. 13.] ->  +11.041  +9.000    2.041   \n",
      "           [ 0.  3. 10. 11.] ->  +0.096  +0.000    0.096   \n",
      "               [0. 0. 0. 9.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  1. 12. 12.] ->  +0.096  +0.000    0.096   \n",
      "               [0. 0. 3. 9.] ->  +1.227  +2.000    0.773   \n",
      "           [ 0.  0.  7. 13.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  3. 14.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  7. 13.] ->  +15.880  +8.000    7.880   \n",
      "               [0. 0. 0. 5.] ->  +14.651  +11.000    3.651   \n",
      "           [ 0.  0.  9. 16.] ->  -0.041  +2.000    2.041   \n",
      "           [ 0.  0.  7. 14.] ->  -0.040  +0.000    0.040   \n",
      "           [ 0.  0.  7. 14.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0.  3. 15.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  5. 12.] ->  +14.116  +12.000    2.116   \n",
      "               [0. 0. 0. 2.] ->  +15.451  +16.000    0.549   \n",
      "           [ 0.  0. 11. 10.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0.  1. 14.] ->  +15.282  +10.000    5.282   \n",
      "           [ 0.  0.  4. 12.] ->  +1.954  +2.000    0.046   \n",
      "           [ 0.  0.  2. 13.] ->  +6.715  +3.000    3.715   \n",
      "           [ 0.  0. 10. 16.] ->  +4.126  +10.000    5.874   \n",
      "           [ 0.  0.  9. 16.] ->  +3.661  +11.000    7.339   \n",
      "           [ 0.  0.  8. 16.] ->  +7.001  +0.000    7.001   \n",
      "           [ 0.  0.  7. 15.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  9. 15.] ->  +0.379  +1.000    0.621   \n",
      "           [ 0.  0.  5. 14.] ->  +10.512  +11.000    0.488   \n",
      "           [ 0.  0.  6. 16.] ->  +14.115  +12.000    2.115   \n",
      "               [0. 0. 0. 9.] ->  +12.321  +14.000    1.679   \n",
      "               [0. 0. 1. 9.] ->  +15.452  +16.000    0.548   \n",
      "               [0. 0. 0. 0.] ->  +0.375  +0.000    0.375   \n",
      "           [ 0.  0.  6. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 10. 16.] ->  +14.115  +16.000    1.885   \n",
      "               [0. 0. 4. 8.] ->  +13.567  +8.000    5.567   \n",
      "           [ 0.  0.  0. 16.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  0.  1. 13.] ->  +15.451  +13.000    2.451   \n",
      "           [ 0.  0.  8. 16.] ->  +1.943  +0.000    1.943   \n",
      "               [0. 0. 0. 3.] ->  +3.596  +0.000    3.596   \n",
      "               [0. 0. 0. 0.] ->  +0.340  +0.000    0.340   \n",
      "           [ 0.  0.  4. 12.] ->  +0.221  +0.000    0.221   \n",
      "               [0. 0. 1. 5.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  7. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  1. 13.] ->  +14.658  +12.000    2.658   \n",
      "           [ 0.  0.  2. 12.] ->  +15.450  +15.000    0.450   \n",
      "           [ 0.  0. 10. 12.] ->  +3.600  +1.000    2.600   \n",
      "           [ 0.  0.  3. 13.] ->  +6.948  +4.000    2.948   \n",
      "           [ 0.  1. 13. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  3. 12.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  1. 14. 14.] ->  +1.939  +15.000    13.061  \n",
      "           [ 0.  0.  9. 12.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  3. 14.  5.] ->  +0.945  +14.000    13.055  \n",
      "               [0. 0. 0. 1.] ->  +15.447  +16.000    0.553   \n",
      "           [ 0.  3. 10. 16.] ->  +9.784  +9.000    0.784   \n",
      "           [ 0.  0.  1. 13.] ->  +15.448  +16.000    0.552   \n",
      "           [ 0.  0.  2. 16.] ->  +11.273  +7.000    4.273   \n",
      "               [0. 0. 0. 1.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  9. 15.] ->  +0.002  +0.000    0.002   \n",
      "               [0. 0. 0. 2.] ->  +15.451  +13.000    2.451   \n",
      "           [ 0.  0. 12. 16.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  4. 16. 15.] ->  +0.377  +1.000    0.623   \n",
      "           [ 0.  0.  0. 13.] ->  +15.727  +13.000    2.727   \n",
      "               [0. 0. 0. 9.] ->  +15.449  +12.000    3.449   \n",
      "               [0. 0. 0. 1.] ->  +4.102  +3.000    1.102   \n",
      "               [0. 0. 1. 9.] ->  +13.584  +15.000    1.416   \n",
      "           [ 0.  0.  0. 11.] ->  +0.375  +0.000    0.375   \n",
      "               [0. 0. 0. 5.] ->  +14.759  +8.000    6.759   \n",
      "               [0. 0. 3. 4.] ->  +4.459  +0.000    4.459   \n",
      "           [ 0.  2.  9. 16.] ->  +3.608  +8.000    4.392   \n",
      "               [0. 0. 0. 9.] ->  +14.163  +15.000    0.837   \n",
      "           [ 0.  0.  2. 10.] ->  +2.423  +3.000    0.577   \n",
      "               [0. 0. 0. 3.] ->  +14.981  +12.000    2.981   \n",
      "           [ 0.  0.  2. 16.] ->  +14.661  +13.000    1.661   \n",
      "           [ 0.  0.  2. 12.] ->  +14.123  +13.000    1.123   \n",
      "           [ 0.  0.  4. 10.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0. 10. 16.] ->  +1.801  +0.000    1.801   \n",
      "               [0. 0. 0. 7.] ->  +0.077  +0.000    0.077   \n",
      "               [0. 0. 0. 6.] ->  +12.509  +15.000    2.491   \n",
      "           [ 0.  0.  2. 14.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  0. 13. 13.] ->  +6.958  +3.000    3.958   \n",
      "           [ 0.  0.  0. 10.] ->  +14.146  +12.000    2.146   \n",
      "               [0. 0. 0. 2.] ->  +14.647  +14.000    0.647   \n",
      "               [0. 0. 0. 1.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0. 12. 16.] ->  +1.805  +0.000    1.805   \n",
      "           [ 0.  0. 13. 16.] ->  +0.378  +0.000    0.378   \n",
      "               [0. 1. 8. 8.] ->  +0.356  +0.000    0.356   \n",
      "           [ 0.  0.  2. 16.] ->  +14.367  +13.000    1.367   \n",
      "           [ 0.  0.  7. 12.] ->  +0.022  +0.000    0.022   \n",
      "           [ 0.  0.  6. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  3. 12.] ->  +14.118  +12.000    2.118   \n",
      "           [ 0.  0.  6. 14.] ->  +7.396  +4.000    3.396   \n",
      "           [ 0.  0.  4. 15.] ->  +0.554  +3.000    2.446   \n",
      "               [0. 0. 0. 7.] ->  +9.259  +16.000    6.741   \n",
      "           [ 0.  0.  8. 16.] ->  +8.940  +8.000    0.940   \n",
      "           [ 0.  0.  6. 14.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  5. 16. 13.] ->  +7.130  +7.000    0.130   \n",
      "           [ 0.  0.  3. 11.] ->  +14.117  +15.000    0.883   \n",
      "               [0. 0. 0. 2.] ->  +15.439  +12.000    3.439   \n",
      "           [ 0.  0.  8. 16.] ->  -0.042  +0.000    0.042   \n",
      "               [0. 0. 0. 7.] ->  +9.562  +15.000    5.438   \n",
      "           [ 0.  0.  1. 14.] ->  +15.455  +16.000    0.545   \n",
      "           [ 0.  0.  8. 16.] ->  +1.906  +0.000    1.906   \n",
      "           [ 0.  1.  9. 14.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  1.  7. 13.] ->  +0.126  +0.000    0.126   \n",
      "           [ 0.  0.  7. 15.] ->  +4.124  +6.000    1.876   \n",
      "           [ 0.  1. 13. 16.] ->  +4.423  +6.000    1.577   \n",
      "               [0. 0. 0. 3.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  3. 12.] ->  -0.043  +1.000    1.043   \n",
      "           [ 0.  0.  4. 12.] ->  +0.095  +0.000    0.095   \n",
      "               [0. 0. 0. 9.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  1. 13.] ->  +15.381  +16.000    0.619   \n",
      "           [ 0.  3. 15. 16.] ->  +0.378  +1.000    0.622   \n",
      "           [ 0.  0.  6. 12.] ->  +1.943  +1.000    0.943   \n",
      "           [ 0.  1. 15. 16.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  8. 12.] ->  +0.136  +3.000    2.864   \n",
      "               [0. 0. 0. 8.] ->  +13.569  +12.000    1.569   \n",
      "           [ 0.  0.  4. 14.] ->  +11.131  +9.000    2.131   \n",
      "           [ 0.  0.  0. 14.] ->  +14.115  +13.000    1.115   \n",
      "           [ 0.  3. 12. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  1. 10.] ->  +15.445  +15.000    0.445   \n",
      "           [ 0.  1. 10. 15.] ->  +0.235  +0.000    0.235   \n",
      "           [ 0.  0.  5. 11.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  2. 15. 16.] ->  +4.423  +15.000    10.577  \n",
      "           [ 0.  0.  9. 16.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  0.  6. 12.] ->  +5.122  +2.000    3.122   \n",
      "               [0. 0. 1. 7.] ->  +2.133  +1.000    1.133   \n",
      "           [ 0.  1. 10. 16.] ->  +13.644  +3.000    10.644  \n",
      "           [ 0.  1.  3. 15.] ->  +8.995  +6.000    2.995   \n",
      "           [ 0.  0. 12. 16.] ->  -0.058  +4.000    4.058   \n",
      "           [ 0.  2. 10. 12.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 10. 14.] ->  +7.396  +13.000    5.604   \n",
      "           [ 0.  0.  3. 15.] ->  +3.658  +1.000    2.658   \n",
      "           [ 0.  1. 13. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  3. 15. 13.] ->  +3.547  +0.000    3.547   \n",
      "           [ 0.  4. 16. 15.] ->  +12.024  +5.000    7.024   \n",
      "           [ 0.  3. 12. 12.] ->  +0.346  +0.000    0.346   \n",
      "           [ 0.  0.  4. 11.] ->  +4.404  +12.000    7.596   \n",
      "           [ 0.  0. 15. 16.] ->  +0.096  +1.000    0.904   \n",
      "               [0. 0. 0. 4.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0.  0. 11.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  1. 14.] ->  +13.623  +14.000    0.377   \n",
      "           [ 0.  0.  5. 16.] ->  +2.681  +11.000    8.319   \n",
      "           [ 0.  0.  7. 16.] ->  +0.097  +2.000    1.903   \n",
      "           [ 0.  0.  2. 16.] ->  +12.142  +7.000    5.142   \n",
      "               [0. 0. 1. 6.] ->  +3.702  +11.000    7.298   \n",
      "           [ 0.  0.  8. 16.] ->  +3.611  +5.000    1.389   \n",
      "           [ 0.  3. 16. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 10. 15.] ->  +2.218  +4.000    1.782   \n",
      "               [0. 0. 0. 1.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0. 13. 13.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  1. 11.] ->  +1.956  +8.000    6.044   \n",
      "               [0. 0. 0. 3.] ->  +15.452  +14.000    1.452   \n",
      "           [ 0.  0.  8. 12.] ->  +14.115  +10.000    4.115   \n",
      "               [0. 0. 0. 6.] ->  +3.600  +0.000    3.600   \n",
      "           [ 0.  0. 15. 13.] ->  +4.691  +1.000    3.691   \n",
      "           [ 0.  0.  1. 12.] ->  +1.717  +6.000    4.283   \n",
      "           [ 0.  0.  7. 16.] ->  +0.378  +0.000    0.378   \n",
      "           [ 0.  1. 11. 10.] ->  +7.409  +13.000    5.591   \n",
      "               [0. 2. 0. 8.] ->  +4.390  +13.000    8.610   \n",
      "           [ 0.  0.  2. 11.] ->  +11.016  +9.000    2.016   \n",
      "           [ 0.  0.  0. 11.] ->  +2.069  +4.000    1.931   \n",
      "           [ 0.  0. 12. 12.] ->  +0.096  +0.000    0.096   \n",
      "               [0. 0. 0. 6.] ->  +14.670  +16.000    1.330   \n",
      "               [0. 0. 0. 1.] ->  +1.949  +0.000    1.949   \n",
      "           [ 0.  0.  3. 13.] ->  +14.116  +10.000    4.116   \n",
      "               [0. 0. 0. 5.] ->  +15.452  +16.000    0.548   \n",
      "               [0. 0. 7. 8.] ->  +11.041  +9.000    2.041   \n",
      "           [ 0.  1. 12. 16.] ->  +4.228  +4.000    0.228   \n",
      "           [ 0.  3. 13. 16.] ->  +14.083  +7.000    7.083   \n",
      "           [ 0.  4. 16. 16.] ->  +12.338  +10.000    2.338   \n",
      "           [ 0.  1.  9. 15.] ->  +0.095  +0.000    0.095   \n",
      "           [ 0.  0.  3. 12.] ->  +2.108  +0.000    2.108   \n",
      "           [ 0.  0.  5. 11.] ->  +13.569  +13.000    0.569   \n",
      "           [ 0.  0.  9. 14.] ->  +10.951  +16.000    5.049   \n",
      "           [ 0.  0.  1. 10.] ->  +15.451  +16.000    0.549   \n",
      "           [ 0.  2. 12. 14.] ->  +12.003  +16.000    3.997   \n",
      "               [0. 0. 0. 3.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  1. 13.] ->  +14.115  +16.000    1.885   \n",
      "           [ 0.  0.  4. 13.] ->  +4.355  +10.000    5.645   \n",
      "           [ 0.  0.  5. 12.] ->  +7.397  +15.000    7.603   \n",
      "           [ 0.  0.  3. 16.] ->  +4.728  +4.000    0.728   \n",
      "               [0. 0. 0. 7.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  5. 13.] ->  +0.055  +0.000    0.055   \n",
      "           [ 0.  0.  8. 13.] ->  -0.068  +1.000    1.068   \n",
      "           [ 0.  4. 12. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  9. 16.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0. 10. 10.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  0.  0. 10.] ->  +14.115  +13.000    1.115   \n",
      "           [ 0.  0.  5. 12.] ->  +4.871  +8.000    3.129   \n",
      "           [ 0.  0.  3. 15.] ->  +7.619  +11.000    3.381   \n",
      "               [0. 0. 1. 9.] ->  +15.884  +16.000    0.116   \n",
      "           [ 0.  0.  3. 15.] ->  +15.544  +16.000    0.456   \n",
      "           [ 0.  5. 16. 15.] ->  +12.317  +16.000    3.683   \n",
      "           [ 0.  1.  9. 15.] ->  +0.917  +0.000    0.917   \n",
      "           [ 0.  0.  5. 12.] ->  -0.000  +3.000    3.000   \n",
      "           [ 0.  2. 12. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  1.  8. 11.] ->  +1.909  +0.000    1.909   \n",
      "           [ 0.  0.  0. 11.] ->  +15.453  +16.000    0.547   \n",
      "           [ 0.  1. 12. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0. 15. 16.] ->  +9.243  +0.000    9.243   \n",
      "           [ 0.  0.  3. 15.] ->  +0.049  +0.000    0.049   \n",
      "           [ 0.  3. 16.  9.] ->  +11.254  +15.000    3.746   \n",
      "           [ 0.  0.  4. 15.] ->  +11.392  +16.000    4.608   \n",
      "               [0. 0. 0. 1.] ->  +0.720  +0.000    0.720   \n",
      "               [0. 0. 0. 0.] ->  +1.943  +4.000    2.057   \n",
      "           [ 0.  0. 13. 14.] ->  +15.460  +15.000    0.460   \n",
      "               [0. 0. 0. 6.] ->  +15.101  +16.000    0.899   \n",
      "           [ 0.  0. 12.  9.] ->  +14.118  +14.000    0.118   \n",
      "           [ 0.  0.  7. 15.] ->  +5.144  +10.000    4.856   \n",
      "           [ 0.  0.  3. 15.] ->  +12.627  +11.000    1.627   \n",
      "           [ 0.  0.  5. 13.] ->  +2.315  +6.000    3.685   \n",
      "           [ 0.  0.  4. 10.] ->  +1.917  +3.000    1.083   \n",
      "               [0. 0. 0. 6.] ->  +14.652  +15.000    0.348   \n",
      "           [ 0.  0.  4. 15.] ->  +12.143  +10.000    2.143   \n",
      "               [0. 0. 5. 4.] ->  +13.475  +14.000    0.525   \n",
      "               [0. 0. 0. 0.] ->  +15.420  +16.000    0.580   \n",
      "           [ 0.  0.  4. 13.] ->  +8.686  +15.000    6.314   \n",
      "           [ 0.  0.  2. 10.] ->  +14.115  +16.000    1.885   \n",
      "               [0. 0. 0. 2.] ->  +15.452  +15.000    0.452   \n",
      "           [ 0.  0.  5. 15.] ->  +14.115  +13.000    1.115   \n",
      "           [ 0.  0. 11. 15.] ->  +2.863  +0.000    2.863   \n",
      "           [ 0.  0.  4. 15.] ->  +14.115  +13.000    1.115   \n",
      "           [ 0.  0.  7. 16.] ->  -0.042  +0.000    0.042   \n",
      "               [0. 0. 0. 0.] ->  +8.766  +16.000    7.234   \n",
      "           [ 0.  2. 15. 16.] ->  +10.655  +15.000    4.345   \n",
      "           [ 0.  0. 10. 13.] ->  +14.115  +11.000    3.115   \n",
      "           [ 0.  2. 13. 16.] ->  +0.522  +0.000    0.522   \n",
      "               [0. 0. 0. 0.] ->  +14.713  +13.000    1.713   \n",
      "           [ 0.  1.  7. 13.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  2. 15. 16.] ->  +4.422  +7.000    2.578   \n",
      "               [0. 0. 0. 8.] ->  +15.452  +16.000    0.548   \n",
      "           [ 0.  0.  3. 15.] ->  +1.986  +12.000    10.014  \n",
      "           [ 0.  0.  0. 14.] ->  +3.465  +3.000    0.465   \n",
      "               [0. 0. 0. 2.] ->  +12.642  +13.000    0.358   \n",
      "           [ 0.  1. 12. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  4. 15.] ->  +7.405  +11.000    3.595   \n",
      "           [ 0.  0.  5. 13.] ->  -0.042  +0.000    0.042   \n",
      "               [0. 0. 0. 7.] ->  +14.661  +15.000    0.339   \n",
      "               [0. 0. 1. 8.] ->  -0.033  +0.000    0.033   \n",
      "           [ 0.  2. 13. 15.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  5. 16.] ->  +14.115  +16.000    1.885   \n",
      "               [0. 0. 0. 8.] ->  +0.074  +0.000    0.074   \n",
      "           [ 0.  0.  2. 13.] ->  -0.042  +0.000    0.042   \n",
      "           [ 0.  2. 15. 16.] ->  +0.096  +0.000    0.096   \n",
      "           [ 0.  0.  3. 12.] ->  +14.182  +7.000    7.182   \n",
      "               [0. 0. 0. 8.] ->  +9.556  +13.000    3.444   \n",
      "           [ 0.  0.  0. 13.] ->  +7.885  +13.000    5.115   \n",
      "           [ 0.  0.  1. 11.] ->  +15.452  +15.000    0.452   \n",
      "           [ 0.  0.  2. 15.] ->  +14.115  +12.000    2.115   \n",
      "           [ 0.  0.  0. 12.] ->  +14.113  +14.000    0.113   \n",
      "           [ 0.  0.  1. 11.] ->  +15.259  +16.000    0.741   \n",
      "           [ 0.  1. 10. 16.] ->  +4.124  +2.000    2.124   \n",
      "           [ 0.  0.  4. 15.] ->  +2.875  +8.000    5.125   \n",
      "           [ 0.  0.  3. 13.] ->  +1.959  +0.000    1.959   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 1.950902432841051\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 1.79261833e-01  1.39315663e-01  2.18116085e-01  2.20044883e-02\n",
      "  -2.93112067e-02 -1.93199542e-01]\n",
      " [ 1.32358863e-02 -4.23708504e-02 -1.15499784e-01 -3.60440168e-01\n",
      "  -2.76587945e-01 -2.27840248e-01]\n",
      " [-3.74228939e-01  4.39946392e-02 -1.24730134e-01  6.12202009e-01\n",
      "  -2.16904570e-01  2.44911582e-02]\n",
      " [-1.29617076e-02 -5.70143160e-01 -3.61389984e-01 -1.65203315e-01\n",
      "   4.30240691e-01  7.10563742e-01]\n",
      " [-5.18576570e-01 -2.29431998e-01  1.64340778e-01  7.49652933e-01\n",
      "   8.23640898e-02  1.68616803e-01]\n",
      " [-1.28853551e-01 -1.68253261e-01 -8.80761595e-01  1.31244289e+00\n",
      "  -4.68526915e-01 -7.79349667e-01]\n",
      " [-5.84713751e-01  4.56161642e-01 -2.15194657e-01 -2.20337804e-01\n",
      "  -2.58980892e-01  2.53042558e-01]\n",
      " [ 5.18413770e-02 -4.02012986e-01  5.80409161e-01 -1.03170942e-01\n",
      "   9.34200066e-01  3.22752382e-01]\n",
      " [ 6.38082563e-02  3.60651359e-01 -2.32195905e-01  4.98784406e-01\n",
      "   2.36050453e-01 -1.82432295e-01]\n",
      " [-3.71613156e-01  1.81913958e-01  3.63441173e-01 -4.92456808e-01\n",
      "   7.30812233e-01  4.31953347e-02]\n",
      " [-6.93831965e-01 -5.15211555e-01  1.42467521e-02  3.53834771e-01\n",
      "   5.08195628e-01  4.44783280e-01]\n",
      " [-5.80971651e-01  2.40407965e-01 -4.78206718e-01  9.00230790e-01\n",
      "   5.30177594e-02  4.22458038e-01]\n",
      " [-6.58539006e-01 -4.59564780e-01  9.14802010e-02 -4.42908826e-01\n",
      "   7.14820151e-01 -2.99712851e-01]\n",
      " [ 5.14440110e-01 -7.63423302e-01 -1.04871682e-01  2.58079580e-01\n",
      "   6.68013278e-03  1.47836473e-01]\n",
      " [ 4.28236199e-01 -7.42269356e-02 -2.89590960e-01  5.99112360e-01\n",
      "   1.28301322e-01  1.97116950e-01]\n",
      " [-3.88849815e-01 -2.14504008e-01  3.83837399e-01  6.22886995e-02\n",
      "   6.02162901e-02  5.07779484e-01]\n",
      " [ 2.27357609e-01 -6.79961715e-02  3.94258442e-02  6.15960816e-02\n",
      "   8.98752787e-03  6.51808672e-02]\n",
      " [ 1.55943314e-01  5.10976090e-01 -3.53944384e-01 -5.49426249e-03\n",
      "  -1.62272767e-01 -4.36348969e-02]\n",
      " [ 9.34027967e-01  5.03965527e-01 -8.46978467e-02 -6.41261109e-01\n",
      "  -4.03148227e-01  1.30174448e-01]\n",
      " [ 2.14248311e-01  4.97239982e-01 -2.40957007e-01 -6.66760446e-01\n",
      "  -1.20133229e+00 -1.76140470e-01]\n",
      " [ 1.12333414e-02 -1.69548616e+00 -4.39767045e-01 -9.73069520e-01\n",
      "  -2.90281892e-01  4.00767063e-01]\n",
      " [ 7.61679595e-01 -1.34825033e+00  4.44659257e-01 -1.37560317e+00\n",
      "   1.52612096e+00  9.56197106e-01]\n",
      " [ 2.53839416e-01 -1.00946771e+00  7.94511204e-01 -4.81386373e-01\n",
      "   1.56779681e-01 -1.64587638e-01]\n",
      " [-4.07368357e-01  2.42620578e-01 -3.00312263e-01 -7.12140950e-02\n",
      "  -1.79851343e-01 -6.70019408e-03]\n",
      " [ 1.70505261e-01  2.24641893e-01  2.46690392e-01 -3.35380801e-01\n",
      "  -3.08907781e-01  8.01036988e-02]\n",
      " [-2.57621593e-01  2.36736086e-01 -9.49673129e-02 -3.11537928e-01\n",
      "  -1.07072384e+00  1.61013150e-01]\n",
      " [ 4.02343991e-01  6.92562911e-02 -4.67997775e-02  1.12475677e-02\n",
      "  -1.02278016e+00  2.73017057e-01]\n",
      " [ 1.13797368e+00 -7.75355355e-01  2.94415940e-02  1.92952872e-01\n",
      "  -8.32483378e-01 -1.85059620e-01]\n",
      " [-5.26059161e-01 -4.32847672e-01 -5.78085144e-01 -5.17371847e-01\n",
      "  -5.88437812e-01  3.46550935e-02]\n",
      " [-1.65892262e-01 -2.93830434e-01 -1.56688668e-01 -7.07966303e-01\n",
      "   5.55283202e-02  4.09366100e-01]\n",
      " [-5.67330346e-01  1.79282544e-01  1.14259230e+00 -1.18495703e+00\n",
      "   7.92041075e-01  1.05890865e+00]\n",
      " [ 7.32321179e-02  4.60346318e-01  1.87318719e-01 -2.30695376e-01\n",
      "  -7.73368404e-01 -3.27126624e-01]\n",
      " [ 1.77684790e-01  8.33519744e-03  6.53746303e-02 -2.28254091e-01\n",
      "  -5.10569106e-02 -1.57855346e-01]\n",
      " [-6.43419423e-01  1.18585185e+00  1.04671125e+00 -6.39305186e-01\n",
      "   6.15722406e-01 -2.80151228e-02]\n",
      " [ 4.57463067e-01  9.04515077e-02  1.45829232e-01  3.55974147e-01\n",
      "  -5.81350822e-01 -2.28403847e-01]\n",
      " [ 1.75295334e-02 -1.02136289e+00  9.13431995e-01  5.31000708e-01\n",
      "   4.85887510e-01  1.72787625e-01]\n",
      " [-2.42487993e-03 -4.61497047e-01  7.97329146e-01  2.88490787e-01\n",
      "  -5.52669716e-01 -6.58755549e-01]\n",
      " [-2.72720024e-01  2.50716801e-01  4.59613064e-02  6.52652835e-01\n",
      "  -9.05685291e-01  4.11296827e-01]\n",
      " [-1.07208133e-02  4.50342039e-01  6.92158210e-01  2.10377566e-02\n",
      "   8.01647185e-01  2.43627045e-01]\n",
      " [-2.88498653e-01  6.58706822e-02  2.17079043e-01 -2.75554330e-01\n",
      "   2.74081154e-01 -1.54952849e-01]\n",
      " [ 8.84210643e-02  2.33527132e-01 -5.50088295e-02 -4.35283014e-02\n",
      "  -7.22467018e-02 -1.20848220e-01]\n",
      " [-4.48854328e-01  5.67954718e-01  3.33994851e-02 -6.96968427e-01\n",
      "  -3.60732701e-01  6.19594533e-02]\n",
      " [ 1.13910664e+00  9.25032678e-02  1.82919911e+00  1.97892774e-01\n",
      "   8.38918753e-01  8.75607588e-02]\n",
      " [-2.67121071e-01  4.19301435e-01  1.57304387e+00 -1.17228928e+00\n",
      "  -4.12776874e-01 -1.08410068e+00]\n",
      " [ 6.86391835e-01  5.71619373e-01  2.32245719e-01  7.03928806e-01\n",
      "  -1.00259380e-01 -6.99894623e-03]\n",
      " [ 7.57038930e-01  4.97300198e-01 -7.55426572e-02  9.69202762e-01\n",
      "  -3.07654413e-02  4.03769149e-01]\n",
      " [ 6.66968457e-01 -3.92812765e-01 -6.36389278e-01  2.69559292e-01\n",
      "  -3.23316585e-02 -3.06822025e-01]\n",
      " [-8.80168337e-02  3.62194629e-01  4.52831667e-01  1.91411753e-04\n",
      "  -9.53367398e-02  8.50591118e-02]\n",
      " [ 9.87286163e-02  6.22970246e-02  2.38883364e-01  3.61300165e-02\n",
      "  -2.04454767e-01  1.81025355e-01]\n",
      " [-1.62596640e-01  3.85489938e-01 -1.09872682e-01 -1.73350323e-01\n",
      "  -4.28376531e-01  3.43250938e-01]\n",
      " [ 5.97109859e-01 -6.04562065e-02  9.76798855e-01  1.04494289e-01\n",
      "   7.56883136e-01 -9.75506459e-01]\n",
      " [-5.98658905e-01 -3.75732223e-03  2.98607202e-01  2.56927700e-01\n",
      "   1.14052764e-01  1.59942462e-01]\n",
      " [-6.14443443e-02  8.41106415e-01 -1.20804597e+00 -1.53636459e+00\n",
      "  -1.29056336e-02 -7.54016991e-01]\n",
      " [ 1.28966045e-01  5.45013047e-01 -2.52665236e-01 -1.65973616e-01\n",
      "   5.91036615e-01 -1.04193560e+00]\n",
      " [-2.87704981e-01 -3.13082832e-01 -3.42612118e-02  4.60951704e-01\n",
      "  -3.05141319e-01  3.89601663e-03]\n",
      " [ 2.82421987e-01 -6.83361687e-02 -2.54709205e-01  3.34599252e-02\n",
      "   1.55305457e-01 -2.40192980e-01]\n",
      " [ 2.88697580e-01 -2.53454009e-01 -2.20008059e-01  2.18868625e-01\n",
      "   2.74208962e-01  2.81992003e-01]\n",
      " [ 1.29795045e-01  2.55217893e-02 -5.88755455e-01  3.34862588e-01\n",
      "   1.88808603e-01  2.82867999e-02]\n",
      " [-8.15346270e-01  4.83798411e-01 -2.41075570e-01  1.31743459e-01\n",
      "   2.99524425e-01  3.77594679e-01]\n",
      " [ 1.19770024e-01  3.30980320e-01 -1.71992435e-01  1.06475439e-01\n",
      "   1.27212898e-01 -1.86142394e-01]\n",
      " [ 1.31284986e+00 -1.80364847e-01 -1.08856382e+00 -1.04841785e+00\n",
      "  -4.57233829e-02 -4.11156026e-02]\n",
      " [ 2.93529453e-01  4.37605791e-02 -2.27776002e-01 -3.33693605e-01\n",
      "  -4.06797292e-01 -5.72807067e-01]\n",
      " [-4.53373202e-01  8.64806666e-01 -3.76069035e-01 -5.50010018e-01\n",
      "   2.00814412e-01  5.42448944e-02]\n",
      " [ 5.44695502e-01  3.05121823e-02 -3.02304083e-01 -2.62286234e-01\n",
      "   1.76403900e-01 -2.82135325e-01]]\n",
      "[[-0.43213474  1.33507729  0.93547547  2.48473919 -0.30855394 -0.45797537\n",
      "  -0.2530833 ]\n",
      " [-0.70372054  0.54245872  1.65441386 -1.91128792  2.61921919  0.88040641\n",
      "  -1.07035417]\n",
      " [ 0.45688142  2.00139287  1.35043869 -1.0879874   0.65539587 -1.06915205\n",
      "  -1.00959364]\n",
      " [ 2.1595369   0.69048399 -0.29887424  0.07275554 -1.57615238  1.0047623\n",
      "   0.91318837]\n",
      " [-1.49403607  1.60642753 -1.02687129 -0.14752531 -1.16155193  2.41549383\n",
      "   0.89007897]\n",
      " [-0.60940221  0.45639231 -1.45927353  1.11057463 -0.51718364 -0.65410551\n",
      "  -3.11529417]]\n",
      "[[-1.64033446 -0.79618619 -3.21511192  1.24365207 -1.40042227  2.45492375\n",
      "   1.82488948  1.15906498  0.59253914 -0.85300923]\n",
      " [ 2.27114602 -1.20446423 -1.21926049 -1.84834884  0.02343446 -1.96244744\n",
      "   3.19731248  1.45239217  0.98269915 -1.35009526]\n",
      " [ 1.07512694  1.26845221 -0.89476146 -2.19044351  0.60033053  0.68454394\n",
      "   1.43902403 -1.65310041  2.58402403 -2.06538079]\n",
      " [-0.52691229  1.33173907 -2.27370432  1.73204503 -2.13201377 -1.01745141\n",
      "  -0.47517692 -1.80217788  1.97087317  2.35621998]\n",
      " [ 1.55286282  1.23633581  0.01961718 -0.96807527  1.54932152  1.69851909\n",
      "   2.13535476 -2.25700437 -3.0071261  -2.18382019]\n",
      " [ 2.4052425  -3.45826081  2.07440804  2.42320646 -2.99231633  2.27521336\n",
      "  -2.04233096  0.80840744 -0.29928175 -0.5903568 ]\n",
      " [-1.74283434  2.43850907  2.14258782  2.73638349 -1.9740445  -0.02334291\n",
      "   1.11342962 -2.59115987  1.49728206 -2.39874699]]\n",
      "\n",
      "intercepts: [array([-0.73410979, -0.04750592,  0.63591406,  0.42958332,  0.15286114,\n",
      "       -0.97586041]), array([-0.79238487, -1.12604122,  1.32025412,  0.18135399,  0.18892296,\n",
      "        0.82308417, -0.39933985]), array([-0.48787508,  0.68293632,  0.20738244, -0.3716042 , -0.19752372,\n",
      "       -0.36383268,  0.22047762,  0.76746396,  1.49608555,  0.34962296])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We don't usually look inside the NNet, but we can: it's open-box modeling...\n",
    "#\n",
    "if True:  # do we want to see all of the parameters?\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [0, 0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 12, 16, 2, 0, 0, 0, 0, 8, 16, 16, 4, 0, 0, 0, 7, 16, 15, 16, 12, 11, 0, 0, 8, 16, 16, 16, 13, 3, 0, 0, 0, 0, 7, 14, 1, 0, 0, 0, 0, 0, 6, 16, 0, 0, 0, 0, 0, 0, 4, 14, 0, 0, 0]\n",
      "nn.predict_proba ==  [[9.76823179e-05 2.93772730e-02 7.54434137e-05 1.57446955e-07\n",
      "  9.61299913e-01 5.64714274e-03 3.39401855e-03 6.08524704e-05\n",
      "  2.49504412e-05 2.25663701e-05]]\n",
      "prediction: [4.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [0,0,0,3,16,3,0,0,0,0,0,12,16,2,0,0,0,0,8,16,16,4,0,0,0,7,16,15,16,12,11,0,0,8,16,16,16,13,3,0,0,0,0,7,14,1,0,0,0,0,0,6,16,0,0,0,0,0,0,4,14,0,0,0]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")     # takes the max (nice to see them all!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIXELS:\n",
    "+ We will create a function that takes in a single input (`n`, the number of a pixel)\n",
    "+ and then runs the procedure that models the prediction of that pixel from the others...\n",
    "+ then returns the overall average absolute error from the test set\n",
    "\n",
    "#### Run for every pixel!\n",
    "+ Then, we will run that function for every pixel, from 0 to 63\n",
    "+ Create a printed table (need not be fancy: plain-printing is great)\n",
    "+ or a graphic/image,\n",
    "+ to show the \"predictability\" of each of the 64 pixels!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9178328032377099"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pixel_value(n): \n",
    "    \"\"\"Takes in an integer that represents the pixel number, and outputs the overall average absolute error from that test set\"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "    # print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "    # construct the correct X_all from the columns we want\n",
    "    # we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "    X_all = np.concatenate( (A[:,0:n], A[:,n+1:]),axis=1)  # columns 1, 2, 3, and 4\n",
    "\n",
    "    # if we wanted all-except-column 1:   X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "    # if we wanted all-except-column 2:   X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "    # if we wanted all-except-column 3:   X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4\n",
    "    # if we wanted all-except-column 4:   X_all = np.concatenate( (A[:,0:4], A[:,5:]),axis=1)  # columns 0, 1, 2, and 3\n",
    "\n",
    "    y_all = A[:,n]            \n",
    "\n",
    "    indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "    # we scramble both X and y, necessarily with the same permutation\n",
    "    X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "    y_all = y_all[indices]              # again...\n",
    "\n",
    "    #\n",
    "    # We next separate into test data and training data ... \n",
    "    #    + We will train on the training data...\n",
    "    #    + We will _not_ look at the testing data to build the model\n",
    "    #\n",
    "    # Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "    #\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "    #\n",
    "    # for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "    #    This is done through the \"StandardScaler\" in scikit-learn\n",
    "    #\n",
    "\n",
    "    USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "    # we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "    if USE_SCALER == True:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    else:\n",
    "        # this one does no scaling!  We still create it to be consistent:\n",
    "        scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "        scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "    scaler   # is now defined and ready to use...\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # Here are our scaled training and testing sets:\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "    y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "    y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "    #\n",
    "    # MLPRegressor predicts _floating-point_ outputs\n",
    "    #\n",
    "\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                        max_iter=1000,          # how many training epochs\n",
    "                        activation=\"tanh\",     # the activation function\n",
    "                        solver='sgd',          # the optimizer\n",
    "                        verbose=False,          # do we want to watch as it trains?\n",
    "                        shuffle=True,          # shuffle each epoch?\n",
    "                        random_state=None,     # use for reproducibility\n",
    "                        learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                        learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "    # print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "    nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "    # print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "    # print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "    # print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")\n",
    "\n",
    "    #\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "\n",
    "    predictions = nn_regressor.predict(X_test_scaled) # all predictions\n",
    "\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    for i in range(len(y_test_scaled)):\n",
    "        pred = predictions[i]\n",
    "        desired = y_test_scaled[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "\n",
    "    return error/len(y_test_scaled)\n",
    "    \n",
    "\n",
    "pixel_value(42)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pixels = []\n",
    "for i in range(len(COLUMNS)-1):\n",
    "    pixels.append(COLUMNS[i])\n",
    "predictability = []\n",
    "for j in range(64):\n",
    "    predictability.append(pixel_value(j))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set_theme(font_scale=0.75)\n",
    "plt.bar(pixels,predictability)\n",
    "plt.title('Pixels and Predictability-Accuracy')\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Predictability-Accuracy')\n",
    "plt.grid(True)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel 27 is the least predictable with an average absolute error of 3.2227083498426716\n"
     ]
    }
   ],
   "source": [
    "output = 0\n",
    "pix = 0\n",
    "\n",
    "for k in range(64):\n",
    "    pixval = pixel_value(k)\n",
    "    if pixval > output:\n",
    "        output = pixval\n",
    "        pix = k\n",
    "\n",
    "print(f\"Pixel {pix} is the least predictable with an average absolute error of {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel 0 is the most predictable with an average absolute error of 0.005406343586112688\n"
     ]
    }
   ],
   "source": [
    "output = 999999999\n",
    "pix = 0\n",
    "\n",
    "for k in range(64):\n",
    "    pixval = pixel_value(k)\n",
    "    if pixval < output:\n",
    "        output = pixval\n",
    "        pix = k\n",
    "\n",
    "print(f\"Pixel {pix} is the most predictable with an average absolute error of {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As shown above, the most predictable pixel is Pix0 and the least predictable pixel is Pix27. These values do make sense. Pixel 0 is at the top left of the image that composes the digit. That pixel will almost always, if not always, be the same value since it doesn't take part in composing the digit. It will stay as the value 0, which outputs a white pixel. On the other hand, pixel 27 is around the middle of the image, indicating that pix27 will almost always, if not always, vary in value. That pixel will almost always take part in composing the digit so that pixel value will vary between digits. Hence, it is the least predictable pixel. \n",
    "\n",
    "##### This means our model will have a much easier time predicting the value for pix0 than for pix27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: the graph above shows different least and most predictable pixel values but we must recall that the pixel value predictability will always vary... especially with permutated values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c8735478a48ff7ef3deb8c421f15aa5d573c59a98bc92eb9b829f28c47b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
